{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q openai anthropic peft sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/majed_alshaibani/Projects/jrcai_corekit/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # suppress requests warnings of InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Projects/ai-content-detection-dataset/venv/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-SLCxlbRQUx70uDw0-4FWaRyCdzh1MpbJgqyLhpK-azT3BlbkFJO-jSkK6IOK1lvShwmUt9DkA6BSpetmL9jjxRtHrvIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messsage_generator = MessageGeneratorFromOpenAIAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Jais outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    llm_response = messsage_generator(messages)\n",
    "    return llm_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لون نجوم السماء يمكن أن يتراوح بين عدة ألوان استنادًا إلى درجة حرارة النجم وعناصره الكيميائية. عمومًا، يمكن تلخيص ألوان النجوم كالتالي:\n",
      "\n",
      "1. **النجوم الزرقاء**: هذه النجوم هي الأكثر حرارة، ودرجات حرارتها تكون أعلى من 10,000 كلفن.\n",
      "\n",
      "2. **النجوم البيضاء**: تقع هذه النجوم ضمن درجة حرارة متوسطة، بين 7,500 و10,000 كلفن.\n",
      "\n",
      "3. **النجوم الصفراء**: مثل شمسنا، التي تبلغ درجة حرارتها حوالي 5,500 كلفن.\n",
      "\n",
      "4. **النجوم البرتقالية**: درجة حرارتها أقل، تتراوح بين 3,500 و5,000 كلفن.\n",
      "\n",
      "5. **النجوم الحمراء**: هي الأبرد بين النجوم، وتكون درجات حرارتها أقل من 3,500 كلفن.\n",
      "\n",
      "التباين في الألوان يعود إلى الاختلافات في درجة حرارة النجم والعناصر الكيميائية التي يتكون منها الضوء الصادر عنه.\n"
     ]
    }
   ],
   "source": [
    "print(get_chat_completion(\n",
    "    messages=[[{\n",
    "            'role': 'user',\n",
    "            'content': 'ما هو لون نجوم السماء؟'\n",
    "        }]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'summary', 'article'],\n",
       "        num_rows: 49603\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('arbml/AraSum')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49603, 49603)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = dataset['train']['article']\n",
    "summaries = dataset['train']['summary']\n",
    "len(articles), len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 33.53730621131786)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for summaries\n",
    "max_summaries_words = max(len(text.split()) for text in summaries)\n",
    "min_summaries_words = min(len(text.split()) for text in summaries)\n",
    "avg_summaries_words = sum(len(text.split()) for text in summaries) / len(summaries)\n",
    "max_summaries_words, min_summaries_words, avg_summaries_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 33, 376.54426143580025)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for articles\n",
    "max_articles_words = max(len(text.split()) for text in articles)\n",
    "min_articles_words = min(len(text.split()) for text in articles)\n",
    "avg_articles_words = sum(len(text.split()) for text in articles) / len(articles)\n",
    "max_articles_words, min_articles_words, avg_articles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36873, 36873)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "considered_articles,considered_summaries = [],[]\n",
    "for article,summary in zip(articles,summaries):\n",
    "    if 100 < len(article.split()) < 500 and 20 < len(summary.split()) < 50:\n",
    "        considered_articles.append(article)\n",
    "        considered_summaries.append(summary)\n",
    "len(considered_articles),len(considered_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = considered_articles[:3000]\n",
    "summaries = considered_summaries[:3000]\n",
    "len(articles),len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we generate articles by polishing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_jsonl(articles, file_path):  \n",
    "    # Append new articles\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for article in articles:\n",
    "            json_object = article\n",
    "            f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def load_articles_from_jsonl(file_path):\n",
    "    articles = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                articles.append(json.loads(line))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58825413c8d46b79f134d863b3a2f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your existing code to generate articles\n",
    "articles_by_polishing_prompts = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"\"\"\n",
    "قم بإعادة صياغة هذه المقالة التالية، مراعيا تدقيقها لغويا ونحويا وبنائيا ومعنى، المقالة كتبت لتنشر في الصحافة.\n",
    "قم بإعادة الصياغة فقط دون إضافة أي تعبيرات أخرى قبل إعادة الصياغة أو بعدها\n",
    "المقالة:\n",
    "{article}\n",
    "    \"\"\".strip()\n",
    "    articles_by_polishing_prompts.append(prompt)\n",
    "\n",
    "\n",
    "# Main process\n",
    "!mkdir -p generated_arabic_datasets/openai/arasum\n",
    "file_path = \"generated_arabic_datasets/openai/arasum/generated_articles_from_polishing.jsonl\"\n",
    "generated_articles_from_polishing = load_articles_from_jsonl(file_path)\n",
    "\n",
    "for i, prompt in tqdm(\n",
    "    enumerate(\n",
    "        articles_by_polishing_prompts[len(generated_articles_from_polishing) :],\n",
    "        start=len(generated_articles_from_polishing),\n",
    "    ),\n",
    "    total=len(articles),\n",
    "    initial=len(generated_articles_from_polishing)\n",
    "):\n",
    "    output_article = {}\n",
    "    generated = get_chat_completion(\n",
    "        [[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]]\n",
    "    )\n",
    "    output_article[\"original_article\"] = articles[i]\n",
    "    output_article[\"original_article_summary\"] = summaries[i]\n",
    "    output_article[\"generated_article\"] = generated\n",
    "    generated_articles_from_polishing.append(output_article)\n",
    "    # Save the generated articles\n",
    "    save_articles_to_jsonl(generated_articles_from_polishing, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
