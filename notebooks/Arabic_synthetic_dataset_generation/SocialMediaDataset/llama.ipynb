{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/majed_alshaibani/Projects/jrcai_corekit/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # suppress requests warnings of InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Projects/ai-content-detection-dataset/venv/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_loader = LLMLoader(\n",
    "    '/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/',\n",
    "    llm_initializer=Llama31Initializer(),\n",
    "    )\n",
    "messsage_generator = MessageGeneratorFromLocalLLM(llm_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Llama outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    llm_response = messsage_generator(messages)\n",
    "    return llm_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12918f32a485471e981281d9b7a7e7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 1/1 [00:04<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "النجوم بيضاء، ولكن يمكن أن تظهر بألوان مختلفة مثل الأحمر أو الأزرق أو الأصفر.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_chat_completion(\n",
    "    messages=[[{\n",
    "            'role': 'user',\n",
    "            'content': 'ما هو لون نجوم السماء؟'\n",
    "        }]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_posts_to_jsonl(articles, file_path):  \n",
    "    # Append new articles\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for post in articles:\n",
    "            json_object = post\n",
    "            f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def load_existing_posts(file_path):\n",
    "    \"\"\"Load existing posts from a JSONL file, if it exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return [json.loads(line) for line in file]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_posts():\n",
    "    posts = []\n",
    "    with open('arabic_datasets/social_media_dataset.json', 'r', encoding='utf-8') as f:\n",
    "        posts = json.load(f)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = load_posts()\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(generation_prompts, generation_type, batch_size=100):\n",
    "    # Define file path for saving posts\n",
    "    os.makedirs(\"generated_arabic_datasets/llama-batched/arabic_social_media_dataset\", exist_ok=True)\n",
    "    file_path = f\"generated_arabic_datasets/llama-batched/arabic_social_media_dataset/{generation_type}_posts_generation.jsonl\"\n",
    "    \n",
    "    # Load already generated posts to resume if script is interrupted\n",
    "    generated_posts = load_existing_posts(file_path)\n",
    "    start_idx = len(generated_posts)\n",
    "    print(f\"Resuming from batch {start_idx // batch_size}...\")\n",
    "\n",
    "    # Prepare batches of prompts starting from the last completed batch\n",
    "    batched_messages = [\n",
    "        [\n",
    "            [{'role': 'user', 'content': prompt}] \n",
    "            for prompt in generation_prompts[i:i+batch_size]\n",
    "        ]\n",
    "        for i in range(0, len(generation_prompts), batch_size)\n",
    "    ]\n",
    "    \n",
    "    # Main process\n",
    "    for batch_idx, batch in enumerate(tqdm(batched_messages[start_idx // batch_size:], desc=\"Processing batches\"), start=start_idx // batch_size):\n",
    "        generated_batch = messsage_generator(batch)\n",
    "        batch_generated_posts = []\n",
    "        \n",
    "        for post in generated_batch:\n",
    "            post = post.strip().strip('<END>').strip().strip('<START>').strip()\n",
    "            end_token_index = post.find('<END>')\n",
    "            if end_token_index > 0:\n",
    "                post = post[:end_token_index]\n",
    "            batch_generated_posts.append(post.strip())\n",
    "        \n",
    "        # batch_generated_posts = generated_batch\n",
    "\n",
    "        # Get the current slice of the posts\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min((batch_idx + 1) * batch_size, len(generation_prompts))\n",
    "        \n",
    "        # Format the results for this batch\n",
    "        batch_results = [\n",
    "            {\n",
    "                \"original_post\": post,\n",
    "                \"generated_post\": generated_post,\n",
    "            }\n",
    "            for post, generated_post in zip(\n",
    "                posts[batch_start:batch_end],\n",
    "                batch_generated_posts\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Extend the accumulated results and save after each batch\n",
    "        generated_posts.extend(batch_results)\n",
    "        save_posts_to_jsonl(generated_posts, file_path)\n",
    "\n",
    "    print(f\"All batches processed and saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate by polishing posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_by_polishing_post_prompt = \"\"\"\n",
    "أنت خبير في تحليل وتحسين منشورات وسائل التواصل الاجتماعي. مهمتك هي تحسين وصقل المنشور المقدم مع الحفاظ على أسلوبه وطابعه الأصلي.\n",
    "\n",
    "المطلوب تحسين المنشور مع مراعاة:\n",
    "- الحفاظ على اللهجة المستخدمة إن وجدت\n",
    "- الإبقاء على التشكيل إن وجد\n",
    "- استخدام مفردات قريبة من المفردات الأصلية\n",
    "- تصحيح الأخطاء الإملائية والنحوية دون تغيير جوهري في الأسلوب\n",
    "- تحسين التماسك والوضوح مع الحفاظ على الطابع العفوي للمنشور\n",
    "- الحفاظ على عدد كلمات مقارب للمنشور الأصلي ما أمكن وحيثما كان مناسباً\n",
    "\n",
    "مهم جداً: يجب الالتزام بالتالي بدقة:\n",
    "- ابدأ المنشور المحسّن مباشرة بعد علامة <START>\n",
    "- انتهِ المنشور المحسّن بعلامة <END>\n",
    "- لا تضف أي تعليقات أو ملاحظات قبل <START> أو بعد <END>\n",
    "- لا تكتب أي عبارات تمهيدية أو ختامية\n",
    "- لا تضف أي نص ليس جزءاً من المنشور المحسّن نفسه\n",
    "- لا تعلق على التحسينات أو التغييرات التي قمت بها\n",
    "\n",
    "المنشور الأصلي:\n",
    "{post}\n",
    "\n",
    "المنشور المحسّن:\n",
    "<START>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_by_polishing_post_prompt.format(post=posts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_by_polishing_posts_prompts = []\n",
    "for post in posts:\n",
    "    prompt = generate_by_polishing_post_prompt.format(post=post)\n",
    "    generate_by_polishing_posts_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generate_by_polishing_posts_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_by_polishing_posts_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch 1750...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da861d1043b46acb26e40d2d93f2a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches processed and saved to generated_arabic_datasets/llama-batched/arabic_social_media_dataset/by_polishing_posts_generation.jsonl\n"
     ]
    }
   ],
   "source": [
    "generate_posts(generation_prompts=generate_by_polishing_posts_prompts, generation_type='by_polishing', batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
