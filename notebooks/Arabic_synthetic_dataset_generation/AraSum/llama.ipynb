{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q openai anthropic peft sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/majed_alshaibani/Projects/jrcai_corekit/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # suppress requests warnings of InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Projects/ai-content-detection-dataset/venv/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_loader = LLMLoader(\n",
    "    '/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/',\n",
    "    llm_initializer=Llama31Initializer(),\n",
    "    )\n",
    "messsage_generator = MessageGeneratorFromLocalLLM(llm_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Jais outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    llm_response = messsage_generator(messages)\n",
    "    return llm_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc2820a31b241288cb8be8743fed186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1]\n",
      "النجوم في السماء بيضاء.\n"
     ]
    }
   ],
   "source": [
    "print(get_chat_completion(\n",
    "    messages=[[{\n",
    "            'role': 'user',\n",
    "            'content': 'ما هو لون نجوم السماء؟'\n",
    "        }]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'summary', 'article'],\n",
       "        num_rows: 49603\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('arbml/AraSum')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49603, 49603)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = dataset['train']['article']\n",
    "summaries = dataset['train']['summary']\n",
    "len(articles), len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 33.53730621131786)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for summaries\n",
    "max_summaries_words = max(len(text.split()) for text in summaries)\n",
    "min_summaries_words = min(len(text.split()) for text in summaries)\n",
    "avg_summaries_words = sum(len(text.split()) for text in summaries) / len(summaries)\n",
    "max_summaries_words, min_summaries_words, avg_summaries_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 33, 376.54426143580025)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for articles\n",
    "max_articles_words = max(len(text.split()) for text in articles)\n",
    "min_articles_words = min(len(text.split()) for text in articles)\n",
    "avg_articles_words = sum(len(text.split()) for text in articles) / len(articles)\n",
    "max_articles_words, min_articles_words, avg_articles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36873, 36873)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "considered_articles,considered_summaries = [],[]\n",
    "for article,summary in zip(articles,summaries):\n",
    "    if 100 < len(article.split()) < 500 and 20 < len(summary.split()) < 50:\n",
    "        considered_articles.append(article)\n",
    "        considered_summaries.append(summary)\n",
    "len(considered_articles),len(considered_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = considered_articles[:3000]\n",
    "summaries = considered_summaries[:3000]\n",
    "len(articles),len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we generate articles by polishing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_jsonl(articles, file_path):  \n",
    "    # Append new articles\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for article in articles:\n",
    "            json_object = article\n",
    "            f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def load_articles_from_jsonl(file_path):\n",
    "    articles = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                articles.append(json.loads(line))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aac8c0b7e1145c3a46aede0d05ba328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 9]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 2]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 6]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 10, 10, 1]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 4]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 9]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 9]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 4]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 9]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 9]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 5]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 6]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 5]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 5]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [6, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 5]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 10]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 8]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 2]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 3]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [6, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 4]\n",
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 7]\n",
      "All batches processed and saved to generated_arabic_datasets/llama-batched/arasum/generated_articles_from_polishing.jsonl\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "articles_by_polishing_prompts = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"\"\"\n",
    "قم بإعادة صياغة هذه المقالة التالية، مراعيا تدقيقها لغويا ونحويا وبنائيا ومعنى، المقالة كتبت لتنشر في الصحافة.\n",
    "قم بإعادة الصياغة فقط دون إضافة أي تعبيرات أخرى قبل إعادة الصياغة أو بعدها\n",
    "المقالة:\n",
    "{article}\n",
    "    \"\"\".strip()\n",
    "    articles_by_polishing_prompts.append(prompt)\n",
    "\n",
    "# Create batches of messages\n",
    "batched_messages = [\n",
    "    [\n",
    "        [{'role': 'user', 'content': prompt}] \n",
    "        for prompt in articles_by_polishing_prompts[i:i+BATCH_SIZE]\n",
    "    ]\n",
    "    for i in range(0, len(articles_by_polishing_prompts), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "# Main process\n",
    "!mkdir -p generated_arabic_datasets/llama-batched/arasum\n",
    "file_path = \"generated_arabic_datasets/llama-batched/arasum/generated_articles_from_polishing.jsonl\"\n",
    "\n",
    "generated_articles_from_polishing = []\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(batched_messages, desc=\"Processing batches\")):\n",
    "    generated_batch = messsage_generator(batch)\n",
    "    \n",
    "    batch_start = batch_idx * BATCH_SIZE\n",
    "    batch_end = min((batch_idx + 1) * BATCH_SIZE, len(articles))\n",
    "    \n",
    "    batch_results = [\n",
    "        {\n",
    "            \"original_article\": article,\n",
    "            \"original_article_summary\": summary,\n",
    "            \"generated_article\": generated_article,\n",
    "        }\n",
    "        for article, summary, generated_article in zip(\n",
    "            articles[batch_start:batch_end],\n",
    "            summaries[batch_start:batch_end],\n",
    "            generated_batch\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    generated_articles_from_polishing.extend(batch_results)\n",
    "    \n",
    "    # Save intermediate results after each batch\n",
    "    save_articles_to_jsonl(generated_articles_from_polishing, file_path)\n",
    "\n",
    "# Final save (this is actually redundant now, but kept for clarity)\n",
    "save_articles_to_jsonl(generated_articles_from_polishing, file_path)\n",
    "\n",
    "print(f\"All batches processed and saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
