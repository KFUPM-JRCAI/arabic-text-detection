# Detection of Machine-Generated Arabic Text in the Era of Large Language Models

[![Paper](https://img.shields.io/badge/Paper-Coming%20Soon-red)]([link-to-be-added](https://arxiv.org/abs/2505.23276))
[![Dataset](https://img.shields.io/badge/ğŸ¤—%20Dataset-Abstracts-blue)](https://huggingface.co/datasets/MagedSaeed/arabic-generated-abstracts)
[![Dataset](https://img.shields.io/badge/ğŸ¤—%20Dataset-Social%20Media-green)](https://huggingface.co/datasets/MagedSaeed/arabic-generated-social-media-posts)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)

This repository contains the official implementation and datasets for the research paper **"Detection of Machine-Generated Arabic Text in the Era of Large Language Models"** by Maged S. Al-Shaibani and Moataz Ahmed.

## ğŸ“‹ Overview

This work represents the **first comprehensive investigation and stylometric analysis** of Arabic machine-generated text detection across multiple llms and generation methods, addressing the critical challenge of distinguishing between human-written and AI-generated Arabic content across multiple domains and generation strategies.

### ğŸ¯ Contributions

- **Multi-dimensional stylometric analysis** of human vs. machine-generated Arabic text
- **Multi-prompt generation framework** across 4 LLMs (ALLaM, Jais, Llama 3.1, GPT-4)
- **High-performance detection systems** achieving up to 99.9% F1-score
- **Cross-domain evaluation** (academic abstracts + social media)
- **Cross-model generalization** studies

## ğŸ—ï¸ Repository Structure

```
arabic_datasets/
    â”œâ”€â”€ arabic_filtered_papers.json
    â””â”€â”€ social_media_dataset.json
generated_arabic_datasets/
    â”œâ”€â”€ allam/
        â”œâ”€â”€ arabic_abstracts_dataset/
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ from_title_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_and_content_abstracts_generation_filtered.jsonl
            â””â”€â”€ from_title_and_content_abstracts_generation.jsonl
        â”œâ”€â”€ arabic_social_media_dataset/
            â”œâ”€â”€ by_polishing_posts_generation_filtered.jsonl
            â””â”€â”€ by_polishing_posts_generation.jsonl
        â””â”€â”€ arasum/
            â””â”€â”€ generated_articles_from_polishing.jsonl
    â”œâ”€â”€ claude/
        â”œâ”€â”€ arabic_abstracts_dataset/
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_abstracts_generation.jsonl
            â””â”€â”€ from_title_and_content_abstracts_generation.jsonl
        â””â”€â”€ arasum/
            â””â”€â”€ generated_articles_from_polishing.jsonl
    â”œâ”€â”€ jais-batched/
        â”œâ”€â”€ arabic_abstracts_dataset/
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ from_title_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_and_content_abstracts_generation_filtered.jsonl
            â””â”€â”€ from_title_and_content_abstracts_generation.jsonl
        â”œâ”€â”€ arabic_social_media_dataset/
            â”œâ”€â”€ by_polishing_posts_generation_filtered.jsonl
            â””â”€â”€ by_polishing_posts_generation.jsonl
        â””â”€â”€ arasum/
            â””â”€â”€ generated_articles_from_polishing.jsonl
    â”œâ”€â”€ llama-batched/
        â”œâ”€â”€ arabic_abstracts_dataset/
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ from_title_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_and_content_abstracts_generation_filtered.jsonl
            â””â”€â”€ from_title_and_content_abstracts_generation.jsonl
        â”œâ”€â”€ arabic_social_media_dataset/
            â”œâ”€â”€ by_polishing_posts_generation_filtered.jsonl
            â””â”€â”€ by_polishing_posts_generation.jsonl
        â””â”€â”€ arasum/
            â””â”€â”€ generated_articles_from_polishing.jsonl
    â””â”€â”€ openai/
        â”œâ”€â”€ arabic_abstracts_dataset/
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ by_polishing_abstracts_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_abstracts_generation_filtered.jsonl
            â”œâ”€â”€ from_title_abstracts_generation.jsonl
            â”œâ”€â”€ from_title_and_content_abstracts_generation_filtered.jsonl
            â””â”€â”€ from_title_and_content_abstracts_generation.jsonl
        â”œâ”€â”€ arabic_social_media_dataset/
            â”œâ”€â”€ by_polishing_posts_generation_filtered.jsonl
            â””â”€â”€ by_polishing_posts_generation.jsonl
        â””â”€â”€ arasum/
            â””â”€â”€ generated_articles_from_polishing.jsonl
hf_export/
    â”œâ”€â”€ abstracts_dataset.py
    â””â”€â”€ social_media_dataset.py
models/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data.py
    â”œâ”€â”€ models.py
    â””â”€â”€ train.py
notebooks/
    â”œâ”€â”€ Arabic_experiments/
        â”œâ”€â”€ ArabicAbstractsDataset/
            â”œâ”€â”€ Arabic_abstracts_dataset_preparation.ipynb
            â”œâ”€â”€ continual_training_of_arasum_detector_on_arabic_abstracts.ipynb
            â”œâ”€â”€ llms_multi_class_arabic_detector.ipynb
            â”œâ”€â”€ train_on_one_model_test_on_others.ipynb
            â”œâ”€â”€ train_on_one_prompt_test_on_others.ipynb
            â””â”€â”€ zero_shot_on_arabic_abstracts_dataset.ipynb
        â”œâ”€â”€ ArabicSocialMediaDataset/
            â”œâ”€â”€ llms_multi_class_arabic_detector.ipynb
            â”œâ”€â”€ prepare_arabic_social_media_dataset.ipynb
            â””â”€â”€ train_on_one_model_test_on_others.ipynb
        â””â”€â”€ AraSum/
            â”œâ”€â”€ AllamWithAraSumTestingOnly.ipynb
            â”œâ”€â”€ arabic_detector_trained_on_all_llms.ipynb
            â”œâ”€â”€ arabic_detector_trained_on_allam.ipynb
            â””â”€â”€ arasum_abstracts_detector.ipynb
    â”œâ”€â”€ Arabic_synthetic_dataset_generation/
        â”œâ”€â”€ AbstractsDataset/
            â”œâ”€â”€ allam.ipynb
            â”œâ”€â”€ analysis_on_the_generated_abstracts.ipynb
            â”œâ”€â”€ claude.ipynb
            â”œâ”€â”€ jais.ipynb
            â”œâ”€â”€ llama.ipynb
            â”œâ”€â”€ openai.ipynb
            â””â”€â”€ top_frequent_words_analysis.ipynb
        â”œâ”€â”€ AraSum/
            â”œâ”€â”€ allam.ipynb
            â”œâ”€â”€ claude.ipynb
            â”œâ”€â”€ jais.ipynb
            â”œâ”€â”€ llama.ipynb
            â””â”€â”€ openai.ipynb
        â””â”€â”€ SocialMediaDataset/
            â”œâ”€â”€ allam.ipynb
            â”œâ”€â”€ analysis_on_the_generated_posts.ipynb
            â”œâ”€â”€ jais.ipynb
            â”œâ”€â”€ llama.ipynb
            â”œâ”€â”€ openai.ipynb
            â””â”€â”€ top_frequent_words_analysis.ipynb
    â””â”€â”€ exploration/
        â”œâ”€â”€ explore_arabic_content_detection_dataset.ipynb
        â””â”€â”€ explore_arbicQA_dataset.ipynb
.gitattributes
.gitignore
LICENSE
README.md
requirements.txt
```

## ğŸ”¬ Research Methodology

### Text Generation Strategies

**Academic Abstracts (3 methods):**
- **Title-only generation**: Free-form generation from paper titles
- **Title+Content generation**: Content-aware generation using paper content
- **Abstract polishing**: Refinement of existing human abstracts

**Social Media Posts (1 method):**
- **Post polishing**: Refinement preserving dialectal expressions

### Models Evaluated

| Model | Size | Focus | Source |
|-------|------|-------|--------|
| **ALLaM** | 7B | Arabic-focused | Open |
| **Jais** | 70B | Arabic-focused | Open |
| **Llama 3.1** | 70B | General | Open |
| **GPT-4** | - | General | Closed |

### Detection Approaches

- **Binary detection**: Human vs. Machine-generated
- **Multi-class detection**: Identify specific LLM
- **Cross-model generalization**: Train on one model, test on others

## ğŸ“Š Spotlight Findings

### Stylometric Insights
- **Reduced vocabulary diversity** in AI-generated text
- **Distinctive word frequency patterns** with steeper drop-offs
- **Model-specific linguistic signatures** enabling identification
- **Domain-specific characteristics** varying between contexts

### Detection Performance

**Academic Abstracts:**
- Binary detection: **99.5-99.9% F1-score**
- Cross-model: **86.4-99.9% F1-score range**
- Multi-class: **94.1-98.2% F1-score per model**

**Social Media:**
- More challenging due to informal nature
- **Cross-domain generalization** issues confirmed
- **Model-specific detectability** variations observed

## ğŸš€ Getting Started

### Prerequisites

```bash
# Python 3.8+
pip install -r requirements.txt
```

### Installation

Make sure to also take a look at the corekit repo (https://github.com/MagedSaeed/llms-corekit) as it is needed in some scripts and notebooks.

```bash
git clone https://github.com/MagedSaeed/arabic-text-detection.git
cd arabic-text-detection

# Install dependencies
pip install -r requirements.txt

# Download datasets (requires Git LFS)
git lfs pull
```

The code was written with practices that support self-explanatory purpuses. You can browse the scripts and notebooks and run them providing the necessary API keys if required.

## ğŸ“ Datasets

### Academic Abstracts
- **Source**: [Algerian Scientific Journals Platform](https://asjp.cerist.dz/)
- **Size**: 8,388 samples across 3 generation methods
- **Period**: 2010-2022 (pre-AI era)
- **Available**: [ğŸ¤— HuggingFace Hub](https://huggingface.co/datasets/MagedSaeed/arabic-generated-abstracts)

### Social Media Posts
- **Source**: BRAD (Book Reviews) + HARD (Hotel Reviews)
- **Size**: 3,318 samples (polishing method)
- **Available**: [ğŸ¤— HuggingFace Hub](https://huggingface.co/datasets/MagedSaeed/arabic-generated-social-media-posts)


## ğŸ”— Related Work

- **Abstracts Data Collection code**: [arabic-dataset](https://github.com/MagedSaeed/arabs-dataset)
- **LLMs corekit**: [LLMs-corekit](https://github.com/MagedSaeed/llms-corekit)

## ğŸ“š Citation

```bibtex
coming soon
```

## ğŸ¢ Institutional Support

This research is supported by:
- **SDAIA-KFUPM Joint Research Center for Artificial Intelligence**

## ğŸ‘¥ Authors

- **Maged S. Al-Shaibani**
- **Moataz Ahmed** - Corresponding Author (moataz.ahmed@kfupm.edu.sa)

*SDAIA-KFUPM Joint Research Center for Artificial Intelligence*  
*King Fahd University of Petroleum and Minerals, Saudi Arabia*


## âš–ï¸ Ethical Considerations

This research is intended to:
- **Improve detection** of machine-generated content
- **Enhance academic integrity** in Arabic contexts
- **Advance Arabic NLP** research capabilities
- **Support information verification** systems

Please use this work responsibly and in accordance with ethical AI principles.

---
