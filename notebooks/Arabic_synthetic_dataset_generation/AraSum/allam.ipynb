{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # suppress requests warnings of InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Allam APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = 'xdv6hDyIRvQapMeJ8WjKkoYA9787nkqdXq358jkh37iUlVnHhrHeBP10l3Fa7hwA3PJjlaZZeDXNe5W6DCXII0Sgkw78dDq7gzbM6vjuKgFNVNjWAiSPsYaYOMAGAm56'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        #\"temperature\": 0.6,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False,\n",
    "        \"model\": \"allam\",\n",
    "        #\"top_p\": 0.98,\n",
    "        \"n\": 1,\n",
    "        \"add_generation_prompt\": True,\n",
    "        \"echo\": False,\n",
    "        \"stop\": \" </s>\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        'https://vllm-v19.allam.ai/v1/chat/completions',\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {ACCESS_TOKEN}\"\n",
    "        },\n",
    "        data=json.dumps(payload),\n",
    "        timeout=150,\n",
    "        verify=False,\n",
    "    )\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        chat_response_data = response.json()\n",
    "        # print(chat_response_data)\n",
    "        # return chat_response_data['choices'][0]['message']['content']\n",
    "        return chat_response_data\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-e0fc83c59353419ab8472405862135c3', 'object': 'chat.completion', 'created': 1730303012, 'model': 'allam', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' لون النجوم في السماء يعتمد على درجة حرارة سطح النجم. النجوم الأكثر سخونة تظهر باللون الأزرق، بينما النجوم الأقل حرارة تظهر باللون الأحمر. النجوم الصفراء هي الأكثر شيوعاً وتمثل النجوم ذات درجات حرارة سطح متوسطة. إذا نظرت إلى السماء ليلاً، سترى مجموعة متنوعة من الألوان بسبب درجات الحرارة المختلفة للنجوم. '}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None}], 'usage': {'prompt_tokens': 15, 'total_tokens': 78, 'completion_tokens': 63}}\n"
     ]
    }
   ],
   "source": [
    "print(get_chat_completion([\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'ما هو لون نجوم السماء؟'\n",
    "    },\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'summary', 'article'],\n",
       "        num_rows: 49603\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('arbml/AraSum')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49603, 49603)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = dataset['train']['article']\n",
    "summaries = dataset['train']['summary']\n",
    "len(articles), len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 33.53730621131786)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for summaries\n",
    "max_summaries_words = max(len(text.split()) for text in summaries)\n",
    "min_summaries_words = min(len(text.split()) for text in summaries)\n",
    "avg_summaries_words = sum(len(text.split()) for text in summaries) / len(summaries)\n",
    "max_summaries_words, min_summaries_words, avg_summaries_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 33, 376.54426143580025)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max words, min words, avg words for articles\n",
    "max_articles_words = max(len(text.split()) for text in articles)\n",
    "min_articles_words = min(len(text.split()) for text in articles)\n",
    "avg_articles_words = sum(len(text.split()) for text in articles) / len(articles)\n",
    "max_articles_words, min_articles_words, avg_articles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36873, 36873)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "considered_articles,considered_summaries = [],[]\n",
    "for article,summary in zip(articles,summaries):\n",
    "    if 100 < len(article.split()) < 500 and 20 < len(summary.split()) < 50:\n",
    "        considered_articles.append(article)\n",
    "        considered_summaries.append(summary)\n",
    "len(considered_articles),len(considered_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = considered_articles[:3000]\n",
    "summaries = considered_summaries[:3000]\n",
    "len(articles),len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we generate articles by polishing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_jsonl(articles, file_path):  \n",
    "    # Append new articles\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for article in articles:\n",
    "            json_object = article\n",
    "            f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def load_articles_from_jsonl(file_path):\n",
    "    articles = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                articles.append(json.loads(line))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2f57f561554409bf7bc7db7ffdeaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 15/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your existing code to generate articles\n",
    "articles_by_polishing_prompts = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"\"\"\n",
    "قم بإعادة صياغة هذه المقالة التالية، مراعيا تدقيقها لغويا ونحويا وبنائيا ومعنى، المقالة كتبت لتنشر في الصحافة.\n",
    "قم بإعادة الصياغة فقط دون إضافة أي تعبيرات أخرى قبل إعادة الصياغة أو بعدها\n",
    "المقالة:\n",
    "{article}\n",
    "    \"\"\".strip()\n",
    "    articles_by_polishing_prompts.append(prompt)\n",
    "\n",
    "\n",
    "# Main process\n",
    "!mkdir -p generated_allam_data\n",
    "file_path = \"generated_arabic_datasets/allam/arasum/generated_articles_from_polishing.jsonl\"\n",
    "generated_articles_from_polishing = load_articles_from_jsonl(file_path)\n",
    "\n",
    "for i, prompt in tqdm(\n",
    "    enumerate(\n",
    "        articles_by_polishing_prompts[len(generated_articles_from_polishing) :],\n",
    "        start=len(generated_articles_from_polishing),\n",
    "    ),\n",
    "    total=len(articles),\n",
    "    initial=len(generated_articles_from_polishing)\n",
    "):\n",
    "    output_article = {}\n",
    "    generated = get_chat_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    output_article[\"original_article\"] = articles[i]\n",
    "    output_article[\"original_article_summary\"] = summaries[i]\n",
    "    output_article[\"generated_article\"] = generated\n",
    "    generated_articles_from_polishing.append(output_article)\n",
    "    # Save the generated articles\n",
    "    save_articles_to_jsonl(generated_articles_from_polishing, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
