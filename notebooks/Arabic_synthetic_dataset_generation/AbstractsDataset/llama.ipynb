{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/majed_alshaibani/Projects/jrcai_corekit/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # suppress requests warnings of InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Projects/ai-content-detection-dataset/venv/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_loader = LLMLoader(\n",
    "    '/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/',\n",
    "    llm_initializer=Llama31Initializer(),\n",
    "    )\n",
    "messsage_generator = MessageGeneratorFromLocalLLM(llm_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Llama outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    llm_response = messsage_generator(messages)\n",
    "    return llm_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/hdd/shared_models/Meta-Llama-3.1-70B-Instruct/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284bc184fc044b25b1e67a365dbc4e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file /hdd/shared_models/Meta-Llama-3.1-70B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 1/1 [00:17<00:00, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لون النجوم يختلف بناءً على درجة حرارة سطحها. تُصنف النجوم إلى عدة فئات بناءً على درجات حرارتها السطحية، وبالتالي تختلف ألوانها. إليك بعض الألوان الشائعة للنجوم:\n",
      "نجوم حارة: تظهر باللون الأبيض أو الأزرق، مثل سيرتيس في كوكبة القيثارة.\n",
      "نجوم معتدلة: تظهر باللون الأبيض أو الأبيض المصفر، مثل الشمس.\n",
      "نجوم باردة: تظهر باللون البرتقالي أو الأحمر، مثل بيتا القيثارة.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_chat_completion(\n",
    "    messages=[[{\n",
    "            'role': 'user',\n",
    "            'content': 'ما هو لون نجوم السماء؟'\n",
    "        }]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_abstracts_to_jsonl(articles, file_path):  \n",
    "    # Append new articles\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for article in articles:\n",
    "            json_object = article\n",
    "            f.write(json.dumps(json_object, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def load_existing_abstracts(file_path):\n",
    "    \"\"\"Load existing abstracts from a JSONL file, if it exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return [json.loads(line) for line in file]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abstracts():\n",
    "    abstracts = []\n",
    "    with open('arabic_datasets/arabic_filtered_papers.json', 'r', encoding='utf-8') as f:\n",
    "        abstracts = json.load(f)\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = load_abstracts()\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstracts(generation_prompts, generation_type, batch_size=100):\n",
    "    # Define file path for saving abstracts\n",
    "    os.makedirs(\"generated_arabic_datasets/llama-batched/arabic_abstracts_dataset\", exist_ok=True)\n",
    "    file_path = f\"generated_arabic_datasets/llama-batched/arabic_abstracts_dataset/{generation_type}_abstracts_generation.jsonl\"\n",
    "    \n",
    "    # Load already generated abstracts to resume if script is interrupted\n",
    "    generated_abstracts = load_existing_abstracts(file_path)\n",
    "    start_idx = len(generated_abstracts)\n",
    "    print(f\"Resuming from batch {start_idx // batch_size}...\")\n",
    "\n",
    "    # Prepare batches of prompts starting from the last completed batch\n",
    "    batched_messages = [\n",
    "        [\n",
    "            [{'role': 'user', 'content': prompt}] \n",
    "            for prompt in generation_prompts[i:i+batch_size]\n",
    "        ]\n",
    "        for i in range(0, len(generation_prompts), batch_size)\n",
    "    ]\n",
    "    \n",
    "    # Main process\n",
    "    for batch_idx, batch in enumerate(tqdm(batched_messages[start_idx // batch_size:], desc=\"Processing batches\"), start=start_idx // batch_size):\n",
    "        generated_batch = messsage_generator(batch)\n",
    "        batch_generated_abstracts = []\n",
    "        \n",
    "        for abstract in generated_batch:\n",
    "            abstract = abstract.strip().strip('<END>').strip().strip('<START>').strip()\n",
    "            end_token_index = abstract.find('<END>')\n",
    "            if end_token_index > 0:\n",
    "                abstract = abstract[:end_token_index]\n",
    "            batch_generated_abstracts.append(abstract.strip())\n",
    "        \n",
    "        # batch_generated_abstracts = generated_batch\n",
    "\n",
    "        # Get the current slice of the abstracts\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min((batch_idx + 1) * batch_size, len(generation_prompts))\n",
    "        \n",
    "        # Format the results for this batch\n",
    "        batch_results = [\n",
    "            {\n",
    "                \"original_abstract\": abstract['arabic_abstract'],\n",
    "                \"generated_abstract\": generated_abstract,\n",
    "            }\n",
    "            for abstract, generated_abstract in zip(\n",
    "                abstracts[batch_start:batch_end],\n",
    "                batch_generated_abstracts\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Extend the accumulated results and save after each batch\n",
    "        generated_abstracts.extend(batch_results)\n",
    "        save_abstracts_to_jsonl(generated_abstracts, file_path)\n",
    "\n",
    "    print(f\"All batches processed and saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_title_prompt = \"\"\"\n",
    "أنت كاتب أكاديمي خبير متخصص في كتابة الملخصات البحثية. مهمتك الأساسية هي إنشاء ملخص موجز ومعبر من فقرة واحدة للورقة البحثية التي عنوانها سيُعطى لك.\n",
    "\n",
    "القواعد الأساسية:\n",
    "1. يجب أن يتراوح الملخص بين 100-150 كلمة بالضبط\n",
    "2. يجب أن يكون الملخص شاملاً ويغطي: الهدف الرئيسي، المنهجية، والنتائج المتوقعة\n",
    "3. لا يُسمح بإنشاء ملخص فارغ تحت أي ظرف\n",
    "4. استخدم أسلوباً علمياً رصيناً يناسب الأوراق البحثية\n",
    "\n",
    "التنسيق المطلوب:\n",
    "- ابدأ الملخص مباشرة بعد علامة <START>\n",
    "- انتهِ بعلامة <END>\n",
    "- لا تضف أي نص خارج هذه العلامات\n",
    "\n",
    "العنوان:\n",
    "{title}\n",
    "\n",
    "الملخص:\n",
    "<START>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنت كاتب أكاديمي خبير متخصص في كتابة الملخصات البحثية. مهمتك الأساسية هي إنشاء ملخص موجز ومعبر من فقرة واحدة للورقة البحثية التي عنوانها سيُعطى لك.\n",
      "\n",
      "القواعد الأساسية:\n",
      "1. يجب أن يتراوح الملخص بين 100-150 كلمة بالضبط\n",
      "2. يجب أن يكون الملخص شاملاً ويغطي: الهدف الرئيسي، المنهجية، والنتائج المتوقعة\n",
      "3. لا يُسمح بإنشاء ملخص فارغ تحت أي ظرف\n",
      "4. استخدم أسلوباً علمياً رصيناً يناسب الأوراق البحثية\n",
      "\n",
      "التنسيق المطلوب:\n",
      "- ابدأ الملخص مباشرة بعد علامة <START>\n",
      "- انتهِ بعلامة <END>\n",
      "- لا تضف أي نص خارج هذه العلامات\n",
      "\n",
      "العنوان:\n",
      "صور عن نظام التعليم عند المرأة الأندلسية\n",
      "\n",
      "الملخص:\n",
      "<START>\n"
     ]
    }
   ],
   "source": [
    "print(generate_from_title_prompt.format(title=abstracts[0]['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_title_prompts = []\n",
    "for abstract in abstracts:\n",
    "    prompt = generate_from_title_prompt.format(title=abstract['title'])\n",
    "    generate_from_title_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أنت كاتب أكاديمي خبير متخصص في كتابة الملخصات البحثية. مهمتك الأساسية هي إنشاء ملخص موجز ومعبر من فقرة واحدة للورقة البحثية التي عنوانها سيُعطى لك.\\n\\nالقواعد الأساسية:\\n1. يجب أن يتراوح الملخص بين 100-150 كلمة بالضبط\\n2. يجب أن يكون الملخص شاملاً ويغطي: الهدف الرئيسي، المنهجية، والنتائج المتوقعة\\n3. لا يُسمح بإنشاء ملخص فارغ تحت أي ظرف\\n4. استخدم أسلوباً علمياً رصيناً يناسب الأوراق البحثية\\n\\nالتنسيق المطلوب:\\n- ابدأ الملخص مباشرة بعد علامة <START>\\n- انتهِ بعلامة <END>\\n- لا تضف أي نص خارج هذه العلامات\\n\\nالعنوان:\\nصور عن نظام التعليم عند المرأة الأندلسية\\n\\nالملخص:\\n<START>',\n",
       " 'أنت كاتب أكاديمي خبير متخصص في كتابة الملخصات البحثية. مهمتك الأساسية هي إنشاء ملخص موجز ومعبر من فقرة واحدة للورقة البحثية التي عنوانها سيُعطى لك.\\n\\nالقواعد الأساسية:\\n1. يجب أن يتراوح الملخص بين 100-150 كلمة بالضبط\\n2. يجب أن يكون الملخص شاملاً ويغطي: الهدف الرئيسي، المنهجية، والنتائج المتوقعة\\n3. لا يُسمح بإنشاء ملخص فارغ تحت أي ظرف\\n4. استخدم أسلوباً علمياً رصيناً يناسب الأوراق البحثية\\n\\nالتنسيق المطلوب:\\n- ابدأ الملخص مباشرة بعد علامة <START>\\n- انتهِ بعلامة <END>\\n- لا تضف أي نص خارج هذه العلامات\\n\\nالعنوان:\\nانهيار دولة الموحدين – دراسة في الخلفيات الثقافية –\\n\\nالملخص:\\n<START>',\n",
       " 'أنت كاتب أكاديمي خبير متخصص في كتابة الملخصات البحثية. مهمتك الأساسية هي إنشاء ملخص موجز ومعبر من فقرة واحدة للورقة البحثية التي عنوانها سيُعطى لك.\\n\\nالقواعد الأساسية:\\n1. يجب أن يتراوح الملخص بين 100-150 كلمة بالضبط\\n2. يجب أن يكون الملخص شاملاً ويغطي: الهدف الرئيسي، المنهجية، والنتائج المتوقعة\\n3. لا يُسمح بإنشاء ملخص فارغ تحت أي ظرف\\n4. استخدم أسلوباً علمياً رصيناً يناسب الأوراق البحثية\\n\\nالتنسيق المطلوب:\\n- ابدأ الملخص مباشرة بعد علامة <START>\\n- انتهِ بعلامة <END>\\n- لا تضف أي نص خارج هذه العلامات\\n\\nالعنوان:\\nتسليح جيش التحرير الوطني عبر الحدود الغربية  خلال الثورة الجزائرية(1954-1962)\\n\\nالملخص:\\n<START>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_title_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11825aad796545e5850db876661488a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [13, 15, 15, 16, 16, 16, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [06:18<00:00, 54.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 15, 16, 16, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:52<00:00, 50.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 16, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:07<00:00, 43.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:18<00:00, 45.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:20<00:00, 45.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 16, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:15<00:00, 45.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 15, 16, 16, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:38<00:00, 48.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:16<00:00, 45.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:22<00:00, 46.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:22<00:00, 46.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [06:11<00:00, 53.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 15, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [06:23<00:00, 54.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:12<00:00, 44.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 15, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:37<00:00, 48.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:52<00:00, 50.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [06:03<00:00, 51.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 15, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:38<00:00, 48.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:39<00:00, 48.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:19<00:00, 45.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:03<00:00, 43.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:43<00:00, 49.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 16, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:15<00:00, 45.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 16, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:26<00:00, 46.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 16, 16, 16, 16, 16, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:17<00:00, 45.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 16, 16, 16, 16, 16, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:31<00:00, 47.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:41<00:00, 48.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 16, 16, 16, 16, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:13<00:00, 44.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 16, 16, 16, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:47<00:00, 49.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [14, 15, 15, 15, 16, 16, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:32<00:00, 47.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [15, 15, 15, 16, 16, 16, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 7/7 [05:06<00:00, 43.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches processed and saved to generated_arabic_datasets/llama-batched/arabic_abstracts_dataset/from_title_abstracts_generation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_abstracts(generation_prompts=generate_from_title_prompts, generation_type='from_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_paper_content_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_title_and_content_prompt = \"\"\"\n",
    "أنت مساعد أكاديمي خبير متخصص في تحليل وإعادة صياغة النصوص العربية. مهمتك هي تحليل العنوان والمحتوى المقدم لك (بحد أقصى {max_paper_content_length} كلمة) وإنشاء ملخص موجز وفعال من 100-150 كلمة.\n",
    "\n",
    "مهمتك الأساسية هي إنشاء ملخص أكاديمي (100-150 كلمة) في جميع الحالات، حتى لو كان النص المقدم غير مفهوم تماماً.\n",
    "\n",
    "القاعدة الذهبية: يجب عليك دائماً إنشاء ملخص أكاديمي محترف، حتى لو كان عليك الاعتماد على العنوان فقط.\n",
    "\n",
    "ملاحظات مهمة:\n",
    "المحتوى المقدم مستخرج من ملف PDF وقد يحتوي على أخطاء كثيرة في التحويل\n",
    "قد تجد بعض المشاكل الشائعة مثل:\n",
    "- تداخل الحروف العربية بشكل كبير\n",
    "- تكرار بعض الحروف بشكل غير منطقي (مثل: السسسلام)\n",
    "- تشويه كبير في بعض الكلمات لدرجة قد تجعلها غير مفهومة\n",
    "- فصل غير صحيح للكلمات\n",
    "- أخطاء في علامات التشكيل\n",
    "- تحويل خاطئ لبعض الحروف المتشابهة مثل (ا/أ/إ) و (ه/ة)\n",
    "- اختفاء بعض الحروف أو استبدالها برموز غير مفهومة\n",
    "\n",
    "إرشادات مهمة:\n",
    "- تجاهل تماماً أي ملخص أو مستخلص موجود في المحتوى المقدم (عادةً ما يكون في بداية البحث). لم نقم بحذفه لأنه من الصعب حذف الملخص الموجود مسبقاً من النص المدخل، إلا أن مهمتك هي تجاهله بشكل كامل وإنشاء ملخص جديد مستقل بناءً على محتوى البحث نفسه. لا تتأثر بالملخص الموجود ولا تعتمد عليه في صياغة الملخص الجديد.\n",
    "- استخدام سياق النص لفهم المعنى المقصود وتصحيح الأخطاء ذهنياً.\n",
    "- التركيز على الأفكار الواضحة والمفهومة في النص.\n",
    "- في حال وجود أجزاء غير مفهومة بسبب التشويه، يمكنك الاعتماد على الأجزاء المفهومة.\n",
    "- في حال كان النص غير مفهوم بشكل كبير، قم بإنشاء ملخص من عنوان البحث فقط، مع التركيز على الموضوع الرئيسي والمجال العلمي والأهداف المتوقعة.\n",
    "\n",
    "المطلوب:\n",
    "- قراءة العنوان والمحتوى بعناية.\n",
    "- فهم الأفكار الرئيسية رغم أخطاء التحويل الكثيرة.\n",
    "- إنشاء ملخص واضح ومتماسك يعكس جوهر البحث.\n",
    "- الالتزام بالأسلوب الأكاديمي الرصين.\n",
    "\n",
    "تنبيهات هامة للملخص:\n",
    "- يجب أن يكون الملخص نصاً متصلاً بدون أسطر فارغة أو فواصل سطرية.\n",
    "- لا تترك الملخص فارغاً تحت أي ظرف.\n",
    "- يجب أن يكون الملخص أكاديمياً بحتاً، يصف محتوى البحث أو موضوعه وأهدافه.\n",
    "- ممنوع منعاً باتاً كتابة عبارات مثل \"النص غير واضح\" أو \"يصعب تقديم ملخص\" أو \"للأسف، النص المقدم غير واضح\" أو أي تعليقات حول جودة النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\n",
    "- في حال عدم وضوح النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\n",
    "- لا تضف مسافات أو أسطر فارغة في بداية أو نهاية الملخص.\n",
    "- اكتب الملخص مباشرة بعد علامة <START> بدون أي مسافات إضافية.\n",
    "\n",
    "تذكر: مهمتك الأساسية هي إنتاج ملخص أكاديمي محترف في جميع الحالات. عدم فهم النص ليس عذراً لعدم إنتاج ملخص - في هذه الحالة، استخدم العنوان لإنشاء ملخص يتناول الموضوع من منظور أكاديمي عام.\n",
    "\n",
    "يرجى البدء مباشرة بعد علامة <START> والانتهاء عند علامة <END>. لا تضف أي نص خارج هذه العلامات.\n",
    "\n",
    "المحتوى:\n",
    "{content}\n",
    "\n",
    "العنوان:\n",
    "{title}\n",
    "\n",
    "الملخص:\n",
    "<START>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنت مساعد أكاديمي خبير متخصص في تحليل وإعادة صياغة النصوص العربية. مهمتك هي تحليل العنوان والمحتوى المقدم لك (بحد أقصى 400 كلمة) وإنشاء ملخص موجز وفعال من 100-150 كلمة.\n",
      "\n",
      "مهمتك الأساسية هي إنشاء ملخص أكاديمي (100-150 كلمة) في جميع الحالات، حتى لو كان النص المقدم غير مفهوم تماماً.\n",
      "\n",
      "القاعدة الذهبية: يجب عليك دائماً إنشاء ملخص أكاديمي محترف، حتى لو كان عليك الاعتماد على العنوان فقط.\n",
      "\n",
      "ملاحظات مهمة:\n",
      "المحتوى المقدم مستخرج من ملف PDF وقد يحتوي على أخطاء كثيرة في التحويل\n",
      "قد تجد بعض المشاكل الشائعة مثل:\n",
      "- تداخل الحروف العربية بشكل كبير\n",
      "- تكرار بعض الحروف بشكل غير منطقي (مثل: السسسلام)\n",
      "- تشويه كبير في بعض الكلمات لدرجة قد تجعلها غير مفهومة\n",
      "- فصل غير صحيح للكلمات\n",
      "- أخطاء في علامات التشكيل\n",
      "- تحويل خاطئ لبعض الحروف المتشابهة مثل (ا/أ/إ) و (ه/ة)\n",
      "- اختفاء بعض الحروف أو استبدالها برموز غير مفهومة\n",
      "\n",
      "إرشادات مهمة:\n",
      "- تجاهل تماماً أي ملخص أو مستخلص موجود في المحتوى المقدم (عادةً ما يكون في بداية البحث). لم نقم بحذفه لأنه من الصعب حذف الملخص الموجود مسبقاً من النص المدخل، إلا أن مهمتك هي تجاهله بشكل كامل وإنشاء ملخص جديد مستقل بناءً على محتوى البحث نفسه. لا تتأثر بالملخص الموجود ولا تعتمد عليه في صياغة الملخص الجديد.\n",
      "- استخدام سياق النص لفهم المعنى المقصود وتصحيح الأخطاء ذهنياً.\n",
      "- التركيز على الأفكار الواضحة والمفهومة في النص.\n",
      "- في حال وجود أجزاء غير مفهومة بسبب التشويه، يمكنك الاعتماد على الأجزاء المفهومة.\n",
      "- في حال كان النص غير مفهوم بشكل كبير، قم بإنشاء ملخص من عنوان البحث فقط، مع التركيز على الموضوع الرئيسي والمجال العلمي والأهداف المتوقعة.\n",
      "\n",
      "المطلوب:\n",
      "- قراءة العنوان والمحتوى بعناية.\n",
      "- فهم الأفكار الرئيسية رغم أخطاء التحويل الكثيرة.\n",
      "- إنشاء ملخص واضح ومتماسك يعكس جوهر البحث.\n",
      "- الالتزام بالأسلوب الأكاديمي الرصين.\n",
      "\n",
      "تنبيهات هامة للملخص:\n",
      "- يجب أن يكون الملخص نصاً متصلاً بدون أسطر فارغة أو فواصل سطرية.\n",
      "- لا تترك الملخص فارغاً تحت أي ظرف.\n",
      "- يجب أن يكون الملخص أكاديمياً بحتاً، يصف محتوى البحث أو موضوعه وأهدافه.\n",
      "- ممنوع منعاً باتاً كتابة عبارات مثل \"النص غير واضح\" أو \"يصعب تقديم ملخص\" أو \"للأسف، النص المقدم غير واضح\" أو أي تعليقات حول جودة النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\n",
      "- في حال عدم وضوح النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\n",
      "- لا تضف مسافات أو أسطر فارغة في بداية أو نهاية الملخص.\n",
      "- اكتب الملخص مباشرة بعد علامة <START> بدون أي مسافات إضافية.\n",
      "\n",
      "تذكر: مهمتك الأساسية هي إنتاج ملخص أكاديمي محترف في جميع الحالات. عدم فهم النص ليس عذراً لعدم إنتاج ملخص - في هذه الحالة، استخدم العنوان لإنشاء ملخص يتناول الموضوع من منظور أكاديمي عام.\n",
      "\n",
      "يرجى البدء مباشرة بعد علامة <START> والانتهاء عند علامة <END>. لا تضف أي نص خارج هذه العلامات.\n",
      "\n",
      "المحتوى:\n",
      "مجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 6< =6 صور عننظام التعليم عند المرأة األندلسية أ- يـمانـي رشـيد ، قسم التاريخ ، جامعة تلمسان . تمهـيد : كثَتا ما ارتبطت ا١تصادر التارخيية يف األندلس خاصة منها كتب الًتاجم والفهرسات والربامج وغَتىا بدراسة حياة العلماء و الرواة والقضاة و الساسة ؛ وقد تطورت ىذه ا١تادة حىت ترك لنا ا١تؤلفون األندلسيون سلسلة متواصلة اٟتلقات من كتب التـراجم كالصلة البن بشكوال ، وصلة الصلة البن الزبَت، والتكملة لكتاب الصلة البن اآلبار، والذيل والتكملة لكتايب ا١توصول و الصلة البن عبد ا١تلك ا١تراكشي إضافة إىل اإلحاطة يف أخبار غرناطة البن ا٠تطيب 1 ،إال أهنا مل تنس أن تشَت يف ثنايا أو باألحرى يف خوامت ىذه ا١تؤلفات إىل فئة ا١ترأة العا١تة2 اليت سامهت يف اإلنتاج الفكري واٟتضاري األندلسي . ومن خال٢تا سنسعى إىل الوقوف على حالة التعليم عند ا١ترأة األندلسية ، وكيف كانت تأخذ فنون العلم ؟ وما مدى إسهامها يف الفكر الًتبوي واإلنتاج الفكري األندلسيُت ؟ مل يقتصر العلم والتعليم يف األندلس على الرجال ، فقد عرفت كذلك ميالد عالـمات أديبات ، وسامهن النساء يف ٣تالس العلم فروين اٟتديث و قرأن على الشيوخ حيث تًتدد بعض أٝتاءىن ، كـما اشتهرت بعض النساء ٔتهارهتن يف الكتابة ونسخ ا١تصاحف و الكتب ا٠تاصة بالفقو وعلوم أخرى ويبعثنها إىل الوراقُت3 ،والالئي بلغن يف قرطبة لوحدىا حوايل مائة وستون امرأة4. كما نبغت\n",
      "مجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 6< =7 األندلسياتيف الشعر منذ عصر اإلمارة األموية حىت فاق عددىن شاعرات ا١تشرق اإلسالمي . كان األندلسيون يـبعـثون بالفتيات إىل ا١تدارس االبتدائية منذ الصغر وبعضهن كـن يواصلن الدراسة يف التعليم العـايل وحيصلن على اإلجازات 5 ، وك ن يدرسن الفقو والقراءات والسنة حىت أن بعضهن تأىلن إىل امتاان التعليم و٦تارستو كمهنة شريفة ، وأحيانا يتبوأن مناصب يف ديوان الكتابة ا١تلكية، فكانت خطوطهن ٚتيلة يف لغة راقية . وإذا كانت كتب الًتاجم قد أمدتنا بعدد من أٝتاء العا١تات يف الفقو واألدب و الشعر الاليت أسهمن يف ازدىار العلوم وتعليمو لبنات جنسهن ، فقد كانت حفصة بنت الحاج الركوني أستاذة وقتها وعلمت النساء يف دار ا١تنصور6. لكن يف ا١تقابل كانت شاياة حيث مل نعثر على نصوص تارخيية تؤكد أن البنات ارتدن الكتاتيب و ا١تدارس إىل جانب الذكور . فكل ما عثرنا عليو ما ورد يف بعض كتب الًتاجم إال بعض اإلشارات اليت تبُت من أخذن العلم يف حلقات التدريس حىت كان ٢تا شأنا يف ذلك كأم الهناء بنت القاضي أبي محمد عبد الحق بن عطية 7 ، وقرأت ريحانة على يد أيب عمر ا١تقري ، وحضرت ٣تالسو خلف\n",
      "\n",
      "العنوان:\n",
      "صور عن نظام التعليم عند المرأة الأندلسية\n",
      "\n",
      "الملخص:\n",
      "<START>\n"
     ]
    }
   ],
   "source": [
    "print(generate_from_title_and_content_prompt.format(title=abstracts[0]['title'],content=' '.join(abstracts[0]['paper_extracted_content'].split(' ')[:max_paper_content_length]),max_paper_content_length=max_paper_content_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_title_and_content_prompts = []\n",
    "for abstract in abstracts:\n",
    "    paper_content = ' '.join(abstract['paper_extracted_content'].split(' ')[:max_paper_content_length])\n",
    "    prompt = generate_from_title_and_content_prompt.format(title=abstract['title'], content=paper_content, max_paper_content_length=max_paper_content_length)\n",
    "    generate_from_title_and_content_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أنت مساعد أكاديمي خبير متخصص في تحليل وإعادة صياغة النصوص العربية. مهمتك هي تحليل العنوان والمحتوى المقدم لك (بحد أقصى 400 كلمة) وإنشاء ملخص موجز وفعال من 100-150 كلمة.\\n\\nمهمتك الأساسية هي إنشاء ملخص أكاديمي (100-150 كلمة) في جميع الحالات، حتى لو كان النص المقدم غير مفهوم تماماً.\\n\\nالقاعدة الذهبية: يجب عليك دائماً إنشاء ملخص أكاديمي محترف، حتى لو كان عليك الاعتماد على العنوان فقط.\\n\\nملاحظات مهمة:\\nالمحتوى المقدم مستخرج من ملف PDF وقد يحتوي على أخطاء كثيرة في التحويل\\nقد تجد بعض المشاكل الشائعة مثل:\\n- تداخل الحروف العربية بشكل كبير\\n- تكرار بعض الحروف بشكل غير منطقي (مثل: السسسلام)\\n- تشويه كبير في بعض الكلمات لدرجة قد تجعلها غير مفهومة\\n- فصل غير صحيح للكلمات\\n- أخطاء في علامات التشكيل\\n- تحويل خاطئ لبعض الحروف المتشابهة مثل (ا/أ/إ) و (ه/ة)\\n- اختفاء بعض الحروف أو استبدالها برموز غير مفهومة\\n\\nإرشادات مهمة:\\n- تجاهل تماماً أي ملخص أو مستخلص موجود في المحتوى المقدم (عادةً ما يكون في بداية البحث). لم نقم بحذفه لأنه من الصعب حذف الملخص الموجود مسبقاً من النص المدخل، إلا أن مهمتك هي تجاهله بشكل كامل وإنشاء ملخص جديد مستقل بناءً على محتوى البحث نفسه. لا تتأثر بالملخص الموجود ولا تعتمد عليه في صياغة الملخص الجديد.\\n- استخدام سياق النص لفهم المعنى المقصود وتصحيح الأخطاء ذهنياً.\\n- التركيز على الأفكار الواضحة والمفهومة في النص.\\n- في حال وجود أجزاء غير مفهومة بسبب التشويه، يمكنك الاعتماد على الأجزاء المفهومة.\\n- في حال كان النص غير مفهوم بشكل كبير، قم بإنشاء ملخص من عنوان البحث فقط، مع التركيز على الموضوع الرئيسي والمجال العلمي والأهداف المتوقعة.\\n\\nالمطلوب:\\n- قراءة العنوان والمحتوى بعناية.\\n- فهم الأفكار الرئيسية رغم أخطاء التحويل الكثيرة.\\n- إنشاء ملخص واضح ومتماسك يعكس جوهر البحث.\\n- الالتزام بالأسلوب الأكاديمي الرصين.\\n\\nتنبيهات هامة للملخص:\\n- يجب أن يكون الملخص نصاً متصلاً بدون أسطر فارغة أو فواصل سطرية.\\n- لا تترك الملخص فارغاً تحت أي ظرف.\\n- يجب أن يكون الملخص أكاديمياً بحتاً، يصف محتوى البحث أو موضوعه وأهدافه.\\n- ممنوع منعاً باتاً كتابة عبارات مثل \"النص غير واضح\" أو \"يصعب تقديم ملخص\" أو \"للأسف، النص المقدم غير واضح\" أو أي تعليقات حول جودة النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- في حال عدم وضوح النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- لا تضف مسافات أو أسطر فارغة في بداية أو نهاية الملخص.\\n- اكتب الملخص مباشرة بعد علامة <START> بدون أي مسافات إضافية.\\n\\nتذكر: مهمتك الأساسية هي إنتاج ملخص أكاديمي محترف في جميع الحالات. عدم فهم النص ليس عذراً لعدم إنتاج ملخص - في هذه الحالة، استخدم العنوان لإنشاء ملخص يتناول الموضوع من منظور أكاديمي عام.\\n\\nيرجى البدء مباشرة بعد علامة <START> والانتهاء عند علامة <END>. لا تضف أي نص خارج هذه العلامات.\\n\\nالمحتوى:\\nمجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 6< =6 صور عننظام التعليم عند المرأة األندلسية أ- يـمانـي رشـيد ، قسم التاريخ ، جامعة تلمسان . تمهـيد : كثَتا ما ارتبطت ا١تصادر التارخيية يف األندلس خاصة منها كتب الًتاجم والفهرسات والربامج وغَتىا بدراسة حياة العلماء و الرواة والقضاة و الساسة ؛ وقد تطورت ىذه ا١تادة حىت ترك لنا ا١تؤلفون األندلسيون سلسلة متواصلة اٟتلقات من كتب التـراجم كالصلة البن بشكوال ، وصلة الصلة البن الزبَت، والتكملة لكتاب الصلة البن اآلبار، والذيل والتكملة لكتايب ا١توصول و الصلة البن عبد ا١تلك ا١تراكشي إضافة إىل اإلحاطة يف أخبار غرناطة البن ا٠تطيب 1 ،إال أهنا مل تنس أن تشَت يف ثنايا أو باألحرى يف خوامت ىذه ا١تؤلفات إىل فئة ا١ترأة العا١تة2 اليت سامهت يف اإلنتاج الفكري واٟتضاري األندلسي . ومن خال٢تا سنسعى إىل الوقوف على حالة التعليم عند ا١ترأة األندلسية ، وكيف كانت تأخذ فنون العلم ؟ وما مدى إسهامها يف الفكر الًتبوي واإلنتاج الفكري األندلسيُت ؟ مل يقتصر العلم والتعليم يف األندلس على الرجال ، فقد عرفت كذلك ميالد عالـمات أديبات ، وسامهن النساء يف ٣تالس العلم فروين اٟتديث و قرأن على الشيوخ حيث تًتدد بعض أٝتاءىن ، كـما اشتهرت بعض النساء ٔتهارهتن يف الكتابة ونسخ ا١تصاحف و الكتب ا٠تاصة بالفقو وعلوم أخرى ويبعثنها إىل الوراقُت3 ،والالئي بلغن يف قرطبة لوحدىا حوايل مائة وستون امرأة4. كما نبغت\\nمجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 6< =7 األندلسياتيف الشعر منذ عصر اإلمارة األموية حىت فاق عددىن شاعرات ا١تشرق اإلسالمي . كان األندلسيون يـبعـثون بالفتيات إىل ا١تدارس االبتدائية منذ الصغر وبعضهن كـن يواصلن الدراسة يف التعليم العـايل وحيصلن على اإلجازات 5 ، وك ن يدرسن الفقو والقراءات والسنة حىت أن بعضهن تأىلن إىل امتاان التعليم و٦تارستو كمهنة شريفة ، وأحيانا يتبوأن مناصب يف ديوان الكتابة ا١تلكية، فكانت خطوطهن ٚتيلة يف لغة راقية . وإذا كانت كتب الًتاجم قد أمدتنا بعدد من أٝتاء العا١تات يف الفقو واألدب و الشعر الاليت أسهمن يف ازدىار العلوم وتعليمو لبنات جنسهن ، فقد كانت حفصة بنت الحاج الركوني أستاذة وقتها وعلمت النساء يف دار ا١تنصور6. لكن يف ا١تقابل كانت شاياة حيث مل نعثر على نصوص تارخيية تؤكد أن البنات ارتدن الكتاتيب و ا١تدارس إىل جانب الذكور . فكل ما عثرنا عليو ما ورد يف بعض كتب الًتاجم إال بعض اإلشارات اليت تبُت من أخذن العلم يف حلقات التدريس حىت كان ٢تا شأنا يف ذلك كأم الهناء بنت القاضي أبي محمد عبد الحق بن عطية 7 ، وقرأت ريحانة على يد أيب عمر ا١تقري ، وحضرت ٣تالسو خلف\\n\\nالعنوان:\\nصور عن نظام التعليم عند المرأة الأندلسية\\n\\nالملخص:\\n<START>',\n",
       " 'أنت مساعد أكاديمي خبير متخصص في تحليل وإعادة صياغة النصوص العربية. مهمتك هي تحليل العنوان والمحتوى المقدم لك (بحد أقصى 400 كلمة) وإنشاء ملخص موجز وفعال من 100-150 كلمة.\\n\\nمهمتك الأساسية هي إنشاء ملخص أكاديمي (100-150 كلمة) في جميع الحالات، حتى لو كان النص المقدم غير مفهوم تماماً.\\n\\nالقاعدة الذهبية: يجب عليك دائماً إنشاء ملخص أكاديمي محترف، حتى لو كان عليك الاعتماد على العنوان فقط.\\n\\nملاحظات مهمة:\\nالمحتوى المقدم مستخرج من ملف PDF وقد يحتوي على أخطاء كثيرة في التحويل\\nقد تجد بعض المشاكل الشائعة مثل:\\n- تداخل الحروف العربية بشكل كبير\\n- تكرار بعض الحروف بشكل غير منطقي (مثل: السسسلام)\\n- تشويه كبير في بعض الكلمات لدرجة قد تجعلها غير مفهومة\\n- فصل غير صحيح للكلمات\\n- أخطاء في علامات التشكيل\\n- تحويل خاطئ لبعض الحروف المتشابهة مثل (ا/أ/إ) و (ه/ة)\\n- اختفاء بعض الحروف أو استبدالها برموز غير مفهومة\\n\\nإرشادات مهمة:\\n- تجاهل تماماً أي ملخص أو مستخلص موجود في المحتوى المقدم (عادةً ما يكون في بداية البحث). لم نقم بحذفه لأنه من الصعب حذف الملخص الموجود مسبقاً من النص المدخل، إلا أن مهمتك هي تجاهله بشكل كامل وإنشاء ملخص جديد مستقل بناءً على محتوى البحث نفسه. لا تتأثر بالملخص الموجود ولا تعتمد عليه في صياغة الملخص الجديد.\\n- استخدام سياق النص لفهم المعنى المقصود وتصحيح الأخطاء ذهنياً.\\n- التركيز على الأفكار الواضحة والمفهومة في النص.\\n- في حال وجود أجزاء غير مفهومة بسبب التشويه، يمكنك الاعتماد على الأجزاء المفهومة.\\n- في حال كان النص غير مفهوم بشكل كبير، قم بإنشاء ملخص من عنوان البحث فقط، مع التركيز على الموضوع الرئيسي والمجال العلمي والأهداف المتوقعة.\\n\\nالمطلوب:\\n- قراءة العنوان والمحتوى بعناية.\\n- فهم الأفكار الرئيسية رغم أخطاء التحويل الكثيرة.\\n- إنشاء ملخص واضح ومتماسك يعكس جوهر البحث.\\n- الالتزام بالأسلوب الأكاديمي الرصين.\\n\\nتنبيهات هامة للملخص:\\n- يجب أن يكون الملخص نصاً متصلاً بدون أسطر فارغة أو فواصل سطرية.\\n- لا تترك الملخص فارغاً تحت أي ظرف.\\n- يجب أن يكون الملخص أكاديمياً بحتاً، يصف محتوى البحث أو موضوعه وأهدافه.\\n- ممنوع منعاً باتاً كتابة عبارات مثل \"النص غير واضح\" أو \"يصعب تقديم ملخص\" أو \"للأسف، النص المقدم غير واضح\" أو أي تعليقات حول جودة النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- في حال عدم وضوح النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- لا تضف مسافات أو أسطر فارغة في بداية أو نهاية الملخص.\\n- اكتب الملخص مباشرة بعد علامة <START> بدون أي مسافات إضافية.\\n\\nتذكر: مهمتك الأساسية هي إنتاج ملخص أكاديمي محترف في جميع الحالات. عدم فهم النص ليس عذراً لعدم إنتاج ملخص - في هذه الحالة، استخدم العنوان لإنشاء ملخص يتناول الموضوع من منظور أكاديمي عام.\\n\\nيرجى البدء مباشرة بعد علامة <START> والانتهاء عند علامة <END>. لا تضف أي نص خارج هذه العلامات.\\n\\nالمحتوى:\\nمجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 60 600 انهيار دولة الموحدين– دراسة في الخلفيات الثقافية – أ - عبد الجبار صديقي المركز الجامعي نور البشير – والية البيض- الملخص يعد العامل الثقايف احد ابرز االسباب اليت يعزى ذلا سقوط الدولة ادلوحدية ، حىت أنو اليقل من حيث التأثَت عن بقية العوامل خاصة السياسية و العسكرية ، فالًتكيب الفكري لعقيدة ابن تومرت اليت اعتربت الركيزة االساسية لقيام دولة ادلوحدين اح توى على تناقضات عقدية و فكرية و تشريعية فادح ة ، جعلتها عرضة لالنتقاد من سلتلف التيارات االجتماعية مبا فيها السلطة احلاكمة اليت ختلت هنائيا عن ادلنهج الفكري التومريت على عهد اخلليفة ادلأ مون يف جتسيد فعلي لتطلعات الفقهاء ادلالكية الذين كانوا اخلصوم االو ائل ذلا سواءا على ادلستوى التشريعي ادلمثل يف الردود العلمية يف شكل مؤلفات أو على مستوى التحركات الثورية ادلسلحة ضد الس لطة اليت قادىا الفقهاء ، ناىيك عن الدور الذي لعبو التيار الصويف يف تقويض اركان الدولة ادلوحدية اليت امتحنت العديد من ادلتصوفة بل و قتلت بعضهم ، ما جعلة يتخندق اىل جانب القو ى ادلناوئة للسلطة و اليت كان ذلا بالغ االثر يف اهنيارىا . Le facteur culturel a ét é l\\'un des principaux qui a causé la chute de la dynastie almohad , alors qu\\'au moins en termes d\\'impact sur le reste des facteurs politiques et militaires . La structure intellectuelle de la doctrine d\\'Ibn Toumert a été considéré comme le principal pilier de la création de l\\'Etat de l\\' almohade se composait de contradictions contractuelles et intellectuelles et lourde\\nمجلة المعارف للبحوث والدراسات التاريخية مجلة دورية دولية محكمة العدد 60 601 législative fait vulnérable à la critique de divers mouvements sociaux , y compris le pouvoir qui a abandonné définitivement la doctri ne d\\'Ibn Toumert pendant le règne du calife Mamoun dans une réalisation effective des aspirations des savants malékites qui étaient ses principaux adversaires que ce soit au représentant de niveau législatif dans les réponses scientifiques sous forme de li vres ou sur le niveau des mouvements révolutionnaires armés contre l\\'autorité dirigée par des fokahas. Sans oublier le rôle joué par le pouvoir mystique saper l\\'état almohade que la plupart des soufis torturés et même tués certains d\\'entre eux Qu\\'est -ce qui les distingue sur le côté de l\\' anti - autorité et les pouvoirs qui ont eu un impact profond dans l\\'effondrement. تمهيد كان النصف األول من القرن السابع ىجري شاىدا على ظهور بوادر التفكك و االهنيار على الدولة\\n\\nالعنوان:\\nانهيار دولة الموحدين – دراسة في الخلفيات الثقافية –\\n\\nالملخص:\\n<START>',\n",
       " 'أنت مساعد أكاديمي خبير متخصص في تحليل وإعادة صياغة النصوص العربية. مهمتك هي تحليل العنوان والمحتوى المقدم لك (بحد أقصى 400 كلمة) وإنشاء ملخص موجز وفعال من 100-150 كلمة.\\n\\nمهمتك الأساسية هي إنشاء ملخص أكاديمي (100-150 كلمة) في جميع الحالات، حتى لو كان النص المقدم غير مفهوم تماماً.\\n\\nالقاعدة الذهبية: يجب عليك دائماً إنشاء ملخص أكاديمي محترف، حتى لو كان عليك الاعتماد على العنوان فقط.\\n\\nملاحظات مهمة:\\nالمحتوى المقدم مستخرج من ملف PDF وقد يحتوي على أخطاء كثيرة في التحويل\\nقد تجد بعض المشاكل الشائعة مثل:\\n- تداخل الحروف العربية بشكل كبير\\n- تكرار بعض الحروف بشكل غير منطقي (مثل: السسسلام)\\n- تشويه كبير في بعض الكلمات لدرجة قد تجعلها غير مفهومة\\n- فصل غير صحيح للكلمات\\n- أخطاء في علامات التشكيل\\n- تحويل خاطئ لبعض الحروف المتشابهة مثل (ا/أ/إ) و (ه/ة)\\n- اختفاء بعض الحروف أو استبدالها برموز غير مفهومة\\n\\nإرشادات مهمة:\\n- تجاهل تماماً أي ملخص أو مستخلص موجود في المحتوى المقدم (عادةً ما يكون في بداية البحث). لم نقم بحذفه لأنه من الصعب حذف الملخص الموجود مسبقاً من النص المدخل، إلا أن مهمتك هي تجاهله بشكل كامل وإنشاء ملخص جديد مستقل بناءً على محتوى البحث نفسه. لا تتأثر بالملخص الموجود ولا تعتمد عليه في صياغة الملخص الجديد.\\n- استخدام سياق النص لفهم المعنى المقصود وتصحيح الأخطاء ذهنياً.\\n- التركيز على الأفكار الواضحة والمفهومة في النص.\\n- في حال وجود أجزاء غير مفهومة بسبب التشويه، يمكنك الاعتماد على الأجزاء المفهومة.\\n- في حال كان النص غير مفهوم بشكل كبير، قم بإنشاء ملخص من عنوان البحث فقط، مع التركيز على الموضوع الرئيسي والمجال العلمي والأهداف المتوقعة.\\n\\nالمطلوب:\\n- قراءة العنوان والمحتوى بعناية.\\n- فهم الأفكار الرئيسية رغم أخطاء التحويل الكثيرة.\\n- إنشاء ملخص واضح ومتماسك يعكس جوهر البحث.\\n- الالتزام بالأسلوب الأكاديمي الرصين.\\n\\nتنبيهات هامة للملخص:\\n- يجب أن يكون الملخص نصاً متصلاً بدون أسطر فارغة أو فواصل سطرية.\\n- لا تترك الملخص فارغاً تحت أي ظرف.\\n- يجب أن يكون الملخص أكاديمياً بحتاً، يصف محتوى البحث أو موضوعه وأهدافه.\\n- ممنوع منعاً باتاً كتابة عبارات مثل \"النص غير واضح\" أو \"يصعب تقديم ملخص\" أو \"للأسف، النص المقدم غير واضح\" أو أي تعليقات حول جودة النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- في حال عدم وضوح النص، قم مباشرة بإنشاء ملخص أكاديمي من العنوان دون الإشارة إلى مشاكل النص.\\n- لا تضف مسافات أو أسطر فارغة في بداية أو نهاية الملخص.\\n- اكتب الملخص مباشرة بعد علامة <START> بدون أي مسافات إضافية.\\n\\nتذكر: مهمتك الأساسية هي إنتاج ملخص أكاديمي محترف في جميع الحالات. عدم فهم النص ليس عذراً لعدم إنتاج ملخص - في هذه الحالة، استخدم العنوان لإنشاء ملخص يتناول الموضوع من منظور أكاديمي عام.\\n\\nيرجى البدء مباشرة بعد علامة <START> والانتهاء عند علامة <END>. لا تضف أي نص خارج هذه العلامات.\\n\\nالمحتوى:\\nمجلة المعارف للبحوث والذراسات التاريخية مجلة دورية دولية محكمة العدد 08 17 تسليح جيش التحرير الوطني عبر الحدود الغربية خالل الثورة الجزائرية(4591-4591) د- الطاهر جبلي- قسم التاريخ - جامعة تلمسان. تمهيـــــد: شككت ت كك اجلهكك د ملااعككالر الةااككدة الكك اككام ثككا اككادة الهكك ة كك ل ة هككا األملىل (7591-7591 ) ل بحث لن صاد ل م يل الهك ة باألسك ح ملالكري ة سككك ا ل الكككدا ل امن اكككا كككن اا كككاقي اودملديككك النكككةاأل ملال ةبألككك (اا ن ككك األملىل ملالهامألكك ملاما ع )بل ككل الككدمل البككا ع الككريم لعبكك ككل ككن صككنلب بككن ب لعألككد مللباس ل ةمل مللزهة شكةي مللمكا ة الععكتةم(ب ا ع)ملحممكد العكةب بكن هألكدم أملل اما ج بل ل جه د أمحد بن ب ك ملل كر هعكاس ملااضكر بنك باإلضكاة إىل دمل حممد ب ضكألا الكريم تت فكمب مهمك ال فعك ألى ل كب اجلبهك ال ةبألك بال عكاملن ملال عكككألي كككا حممكككد العكككةب بكككن هألكككدم األ ضكككأل انكككةملد إ كككداد الهككك ة بالعككك ح ملالككري ة الككريم عككدت عااكك بعككد ككاال 7591 ككن كك ل ال فنككاط اوألكك ملنكككبتات الكككدفلل ال جألعككك ألتر ل لم ألكككات اإل كككداد باألسككك ح ملالكككري ة الكككريم لةة كك الهكك ة ال حةيةيكك ل ككب اجلبه ككا الريفكك ملالبحةيفكك امن اككا ككن ا الككد لعككتةي لألككك لا كككداد ل قكككةاب ملتككك م ململجكككد ملال كككا، مث ا الكككد مل ةا كككز دملديككك ل م ين ل النةق ملال ةب ل صل إىل اا ات ا ل ال اليات الدا أل ب اسن ا اةل ل ع ألى لر عال مل دمةات بةي أمل ن ط إ داد حبةي تةب شبت ال فع ألى ل اما ج (أمل با ملاانةق العةب) ببعض اا امئ ل ال ةب اجلزااةم ملالع ا ل اا ةبأل . مل امن اا ن ااادة ال ا طمأل اا ةةة س حاملل ن ك ل هكري الد اسك الرت ألكز ل ككب لم ألككات إ ككداد الهكك ة باألسكك ح ملالككري ة ل ككب اجلبهكك الريفكك ككا ككد أهككل ااعككككال ملاامككككةات لككككر اوككككدملد ال ةبألكككك الكككك شككككت اا اةككككري اوعاسكككك ل هةيكككك األس ح ال اد ن ملأمل با ملاا ةب.\\nمجلة المعارف للبحوث والذراسات التاريخية مجلة دورية دولية محكمة العدد 08 17 1- :)الجبهة البريَّــــــة (البعد التاريخي واألهمية اإلستراتجية مالككك اجلبهككك الريفككك سكككب اا تا طمألاكككا ا مككك كككا ال اجهككك البحةيفككك صككك ص لم ألككك إ داد اله ة بالع ح ملقةق هتةيب إىل اا كات ا ل اا كاقي الدا\\n\\nالعنوان:\\nتسليح جيش التحرير الوطني عبر الحدود الغربية  خلال الثورة الجزائرية(1954-1962)\\n\\nالملخص:\\n<START>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_title_and_content_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3804e2dfe99460eb8be2c9fa705722d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:04<00:00, 61.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:33<00:00, 71.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:28<00:00, 69.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:21<00:00, 47.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:44<00:00, 74.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:54<00:00, 78.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:25<00:00, 68.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:20<00:00, 66.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:14<00:00, 44.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:06<00:00, 62.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:39<00:00, 53.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:45<00:00, 55.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:40<00:00, 53.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:17<00:00, 68.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:16<00:00, 45.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:53<00:00, 57.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:29<00:00, 49.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:30<00:00, 50.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:18<00:00, 46.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:08<00:00, 62.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:55<00:00, 58.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:18<00:00, 66.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:13<00:00, 64.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:38<00:00, 52.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:18<00:00, 46.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:39<00:00, 53.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:33<00:00, 51.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:21<00:00, 67.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:58<00:00, 59.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:26<00:00, 48.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:00<00:00, 60.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:08<00:00, 62.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 5, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [04:06<00:00, 82.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:38<00:00, 52.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:51<00:00, 57.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:30<00:00, 50.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:57<00:00, 58.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:38<00:00, 72.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:45<00:00, 55.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:31<00:00, 50.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:34<00:00, 51.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:22<00:00, 67.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:25<00:00, 48.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:24<00:00, 68.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:28<00:00, 69.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:18<00:00, 46.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:48<00:00, 56.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:36<00:00, 52.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 5/5 [03:49<00:00, 45.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 4, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:47<00:00, 56.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:26<00:00, 48.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:35<00:00, 51.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:33<00:00, 51.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:00<00:00, 60.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:26<00:00, 48.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:04<00:00, 41.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:20<00:00, 46.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:28<00:00, 49.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:12<00:00, 64.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:51<00:00, 57.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:25<00:00, 48.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:40<00:00, 53.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 5/5 [05:30<00:00, 66.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 2, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 6/6 [06:44<00:00, 67.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:18<00:00, 46.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:52<00:00, 56.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:23<00:00, 67.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:36<00:00, 52.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:14<00:00, 44.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 1, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 6/6 [06:40<00:00, 66.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 3, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:39<00:00, 54.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:07<00:00, 62.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:55<00:00, 58.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:18<00:00, 46.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:24<00:00, 68.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:01<00:00, 40.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:15<00:00, 45.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:01<00:00, 40.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:47<00:00, 55.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:28<00:00, 49.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:56<00:00, 58.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:19<00:00, 46.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:04<00:00, 62.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:09<00:00, 64.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:05<00:00, 61.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:50<00:00, 56.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 5, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:59<00:00, 59.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:09<00:00, 64.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:57<00:00, 58.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:06<00:00, 63.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:23<00:00, 47.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:10<00:00, 65.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:59<00:00, 59.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:57<00:00, 59.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:46<00:00, 55.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:07<00:00, 62.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:30<00:00, 50.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:27<00:00, 49.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:09<00:00, 63.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:16<00:00, 65.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:43<00:00, 54.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:09<00:00, 63.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:54<00:00, 58.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:26<00:00, 48.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 1, 1, 1, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 8/8 [06:40<00:00, 50.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 5/5 [04:51<00:00, 58.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 5/5 [04:13<00:00, 50.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:27<00:00, 49.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:16<00:00, 65.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [04:27<00:00, 89.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:01<00:00, 60.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:14<00:00, 44.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:31<00:00, 50.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 1, 1, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 5/5 [04:47<00:00, 57.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:57<00:00, 58.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:33<00:00, 51.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:24<00:00, 48.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:50<00:00, 56.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:54<00:00, 87.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:56<00:00, 58.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:12<00:00, 64.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:04<00:00, 62.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:59<00:00, 59.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:27<00:00, 69.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:09<00:00, 47.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:58<00:00, 59.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:30<00:00, 50.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:51<00:00, 57.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:51<00:00, 77.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:54<00:00, 58.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:23<00:00, 47.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:38<00:00, 52.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:48<00:00, 54.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:31<00:00, 50.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:38<00:00, 52.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:01<00:00, 60.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:09<00:00, 63.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:41<00:00, 53.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:00<00:00, 60.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:55<00:00, 57.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:57<00:00, 59.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:17<00:00, 45.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:16<00:00, 45.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:43<00:00, 51.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:44<00:00, 52.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:20<00:00, 46.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:08<00:00, 64.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:43<00:00, 74.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:52<00:00, 57.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:31<00:00, 50.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:08<00:00, 42.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:29<00:00, 49.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:10<00:00, 43.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:36<00:00, 52.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:00<00:00, 60.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:09<00:00, 63.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:38<00:00, 72.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:31<00:00, 50.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:24<00:00, 48.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:25<00:00, 48.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:15<00:00, 65.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:01<00:00, 60.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:28<00:00, 49.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:49<00:00, 54.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:03<00:00, 41.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:34<00:00, 51.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:54<00:00, 57.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:06<00:00, 63.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:00<00:00, 60.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:15<00:00, 45.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:39<00:00, 53.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:08<00:00, 42.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:53<00:00, 56.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:25<00:00, 48.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:13<00:00, 64.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:45<00:00, 82.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [01:54<00:00, 38.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:14<00:00, 44.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:57<00:00, 59.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:59<00:00, 59.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [01:51<00:00, 55.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:27<00:00, 49.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:43<00:00, 54.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:09<00:00, 43.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:52<00:00, 57.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:31<00:00, 70.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:45<00:00, 55.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 3, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:52<00:00, 58.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 3, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:32<00:00, 53.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 3, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:31<00:00, 52.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [2, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:53<00:00, 57.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:41<00:00, 53.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:13<00:00, 44.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:00<00:00, 60.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:50<00:00, 76.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:02<00:00, 60.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:29<00:00, 69.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:19<00:00, 66.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 51.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:38<00:00, 52.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:43<00:00, 74.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:07<00:00, 62.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:17<00:00, 65.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:11<00:00, 63.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:47<00:00, 83.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:02<00:00, 61.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:22<00:00, 71.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:16<00:00, 65.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [04:04<00:00, 81.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:53<00:00, 77.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:04<00:00, 61.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:27<00:00, 49.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:47<00:00, 55.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:31<00:00, 70.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:26<00:00, 48.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:24<00:00, 48.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:39<00:00, 53.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:52<00:00, 57.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:53<00:00, 57.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:00<00:00, 60.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:02<00:00, 60.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:04<00:00, 61.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:57<00:00, 59.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:35<00:00, 51.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:46<00:00, 55.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:32<00:00, 50.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:06<00:00, 63.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:17<00:00, 49.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:47<00:00, 56.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:41<00:00, 53.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:34<00:00, 51.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:48<00:00, 56.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:38<00:00, 79.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:43<00:00, 54.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:39<00:00, 53.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 3, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [03:25<00:00, 51.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:24<00:00, 48.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:06<00:00, 42.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:01<00:00, 60.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:33<00:00, 51.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:01<00:00, 60.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:10<00:00, 65.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:37<00:00, 52.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:43<00:00, 81.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:45<00:00, 55.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [04:04<00:00, 81.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:51<00:00, 57.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:12<00:00, 66.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 2/2 [02:07<00:00, 63.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:45<00:00, 55.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:41<00:00, 53.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:34<00:00, 51.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:07<00:00, 62.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:44<00:00, 54.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:16<00:00, 45.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:22<00:00, 47.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:22<00:00, 47.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:03<00:00, 61.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:07<00:00, 62.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:40<00:00, 53.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:27<00:00, 49.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:40<00:00, 53.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 4, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [02:53<00:00, 43.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:53<00:00, 57.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 5, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:42<00:00, 54.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:19<00:00, 46.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [02:21<00:00, 47.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:12<00:00, 64.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [03:28<00:00, 69.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [1, 3, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 4/4 [05:54<00:00, 88.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [4, 4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 3/3 [05:09<00:00, 103.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches processed and saved to generated_arabic_datasets/llama-batched/arabic_abstracts_dataset/from_title_and_content_abstracts_generation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_abstracts(generation_prompts=generate_from_title_and_content_prompts, generation_type='from_title_and_content', batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate by polishing abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_by_polishing_abstract_prompt = \"\"\"\n",
    "أنت كاتب أكاديمي خبير. مهمتك هي تحسين وصقل الملخص المقدم مع مراعاة عنوان البحث.\n",
    "\n",
    "المطلوب إنشاء ملخص موجز وفعال من فقرة واحدة متماسكة، دون فواصل سطرية أو حروف غير ضرورية. يجب أن يتراوح الملخص بين 100-150 كلمة وأن يتناسق بشكل جيد مع العنوان.\n",
    "\n",
    "مهم: قم بإنشاء الملخص المحسّن فقط. ابدأ مباشرة بعد علامة <START> وانتهِ بعلامة <END>. لا تضِف أي كلمات قبل <START> أو بعد <END>. لا تضِف أي عبارات تمهيدية أو جمل ختامية أو أي نص ليس جزءاً من محتوى الملخص الأساسي.\n",
    "\n",
    "إرشادات التحسين:\n",
    "\n",
    "- تحسين الصياغة اللغوية والأسلوب الأكاديمي\n",
    "- التأكد من التسلسل المنطقي للأفكار\n",
    "- تعزيز الترابط بين العنوان والملخص\n",
    "- الحفاظ على المعلومات الأساسية مع تحسين طريقة عرضها\n",
    "- استخدام المصطلحات الأكاديمية المناسبة\n",
    "\n",
    "العنوان:\n",
    "{title}\n",
    "\n",
    "الملخص الأصلي:\n",
    "{abstract}\n",
    "\n",
    "الملخص المحسّن:\n",
    "<START>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنت كاتب أكاديمي خبير. مهمتك هي تحسين وصقل الملخص المقدم مع مراعاة عنوان البحث.\n",
      "\n",
      "المطلوب إنشاء ملخص موجز وفعال من فقرة واحدة متماسكة، دون فواصل سطرية أو حروف غير ضرورية. يجب أن يتراوح الملخص بين 100-150 كلمة وأن يتناسق بشكل جيد مع العنوان.\n",
      "\n",
      "مهم: قم بإنشاء الملخص المحسّن فقط. ابدأ مباشرة بعد علامة <START> وانتهِ بعلامة <END>. لا تضِف أي كلمات قبل <START> أو بعد <END>. لا تضِف أي عبارات تمهيدية أو جمل ختامية أو أي نص ليس جزءاً من محتوى الملخص الأساسي.\n",
      "\n",
      "إرشادات التحسين:\n",
      "\n",
      "- تحسين الصياغة اللغوية والأسلوب الأكاديمي\n",
      "- التأكد من التسلسل المنطقي للأفكار\n",
      "- تعزيز الترابط بين العنوان والملخص\n",
      "- الحفاظ على المعلومات الأساسية مع تحسين طريقة عرضها\n",
      "- استخدام المصطلحات الأكاديمية المناسبة\n",
      "\n",
      "العنوان:\n",
      "صور عن نظام التعليم عند المرأة الأندلسية\n",
      "\n",
      "الملخص الأصلي:\n",
      "كثيرا ما ارتبطت المصادر التاريخية في الأندلس خاصة منها كتب التراجم والفهرسات والبرامج وغيرها بدراسة حياة العلماء والرواة والقضاة والساسة ؛ وقد تطورت هذه المادة حتى ترك لنا المؤلفون الأندلسيون سلسلة متواصلة الحلقات من كتب التـراجم كالصلة لابن بشكوال ، وصلة الصلة لابن الزبير، والتكملة لكتاب الصلة لابن الآبار، والذيل والتكملة لكتابي الموصول والصلة لابن عبد الملك المراكشي إضافة إلى الإحاطة في أخبار غرناطة لابن الخطيب ، إلا أنها لم تنس أن تشير في ثنايا أو بالأحرى في خواتم هذه المؤلفات إلى فئة المرأة العالمة التي ساهمت في الإنتاج الفكري والحضاري الأندلسي. ومن خلالها سنسعى إلى الوقوف على حالة التعليم عند المرأة الأندلسية ، وكيف كانت تأخذ فنون العلم. وما مدى إسهامها في الفكر التربوي والإنتاج الفكري الأندلسيين ؟.\n",
      "\n",
      "الملخص المحسّن:\n",
      "<START>\n"
     ]
    }
   ],
   "source": [
    "print(generate_by_polishing_abstract_prompt.format(title=abstracts[0]['title'],abstract=abstracts[0]['arabic_abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_by_polishing_abstract_prompts = []\n",
    "for abstract in abstracts:\n",
    "    prompt = generate_by_polishing_abstract_prompt.format(title=abstract['title'], abstract=abstract['arabic_abstract'])\n",
    "    generate_by_polishing_abstract_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generate_by_polishing_abstract_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أنت كاتب أكاديمي خبير. مهمتك هي تحسين وصقل الملخص المقدم مع مراعاة عنوان البحث.\\n\\nالمطلوب إنشاء ملخص موجز وفعال من فقرة واحدة متماسكة، دون فواصل سطرية أو حروف غير ضرورية. يجب أن يتراوح الملخص بين 100-150 كلمة وأن يتناسق بشكل جيد مع العنوان.\\n\\nمهم: قم بإنشاء الملخص المحسّن فقط. ابدأ مباشرة بعد علامة <START> وانتهِ بعلامة <END>. لا تضِف أي كلمات قبل <START> أو بعد <END>. لا تضِف أي عبارات تمهيدية أو جمل ختامية أو أي نص ليس جزءاً من محتوى الملخص الأساسي.\\n\\nإرشادات التحسين:\\n\\n- تحسين الصياغة اللغوية والأسلوب الأكاديمي\\n- التأكد من التسلسل المنطقي للأفكار\\n- تعزيز الترابط بين العنوان والملخص\\n- الحفاظ على المعلومات الأساسية مع تحسين طريقة عرضها\\n- استخدام المصطلحات الأكاديمية المناسبة\\n\\nالعنوان:\\nصور عن نظام التعليم عند المرأة الأندلسية\\n\\nالملخص الأصلي:\\nكثيرا ما ارتبطت المصادر التاريخية في الأندلس خاصة منها كتب التراجم والفهرسات والبرامج وغيرها بدراسة حياة العلماء والرواة والقضاة والساسة ؛ وقد تطورت هذه المادة حتى ترك لنا المؤلفون الأندلسيون سلسلة متواصلة الحلقات من كتب التـراجم كالصلة لابن بشكوال ، وصلة الصلة لابن الزبير، والتكملة لكتاب الصلة لابن الآبار، والذيل والتكملة لكتابي الموصول والصلة لابن عبد الملك المراكشي إضافة إلى الإحاطة في أخبار غرناطة لابن الخطيب ، إلا أنها لم تنس أن تشير في ثنايا أو بالأحرى في خواتم هذه المؤلفات إلى فئة المرأة العالمة التي ساهمت في الإنتاج الفكري والحضاري الأندلسي. ومن خلالها سنسعى إلى الوقوف على حالة التعليم عند المرأة الأندلسية ، وكيف كانت تأخذ فنون العلم. وما مدى إسهامها في الفكر التربوي والإنتاج الفكري الأندلسيين ؟.\\n\\nالملخص المحسّن:\\n<START>',\n",
       " 'أنت كاتب أكاديمي خبير. مهمتك هي تحسين وصقل الملخص المقدم مع مراعاة عنوان البحث.\\n\\nالمطلوب إنشاء ملخص موجز وفعال من فقرة واحدة متماسكة، دون فواصل سطرية أو حروف غير ضرورية. يجب أن يتراوح الملخص بين 100-150 كلمة وأن يتناسق بشكل جيد مع العنوان.\\n\\nمهم: قم بإنشاء الملخص المحسّن فقط. ابدأ مباشرة بعد علامة <START> وانتهِ بعلامة <END>. لا تضِف أي كلمات قبل <START> أو بعد <END>. لا تضِف أي عبارات تمهيدية أو جمل ختامية أو أي نص ليس جزءاً من محتوى الملخص الأساسي.\\n\\nإرشادات التحسين:\\n\\n- تحسين الصياغة اللغوية والأسلوب الأكاديمي\\n- التأكد من التسلسل المنطقي للأفكار\\n- تعزيز الترابط بين العنوان والملخص\\n- الحفاظ على المعلومات الأساسية مع تحسين طريقة عرضها\\n- استخدام المصطلحات الأكاديمية المناسبة\\n\\nالعنوان:\\nانهيار دولة الموحدين – دراسة في الخلفيات الثقافية –\\n\\nالملخص الأصلي:\\nيعد العامل الثقافي احد ابرز الاسباب التي يعزى لها سقوط الدولة الموحدية ، حتى أنه لايقل من حيث التأثير عن بقية العوامل خاصة السياسية و العسكرية ، فالتركيب الفكري لعقيدة ابن تومرت التي اعتبرت الركيزة الاساسية لقيام دولة الموحدين احتوى على تناقضات عقدية و فكرية و تشريعية فادحة ، جعلتها عرضة للانتقاد من مختلف التيارات الاجتماعية بما فيها السلطة الحاكمة التي تخلت نهائيا عن المنهج الفكري التومرتي على عهد الخليفة المأمون في تجسيد فعلي لتطلعات الفقهاء المالكية الذين كانوا الخصوم الاوائل لها سواءا على المستوى التشريعي الممثل في الردود العلمية في شكل مؤلفات أو على مستوى التحركات الثورية المسلحة ضد السلطة التي قادها الفقهاء ، ناهيك عن الدور الذي لعبه التيار الصوفي في تقويض اركان الدولة الموحدية التي امتحنت العديد من المتصوفة بل و قتلت بعضهم ، ما جعلة يتخندق الى جانب القوى المناوئة للسلطة و التي كان لها بالغ الاثر في انهيارها.\\n\\nالملخص المحسّن:\\n<START>',\n",
       " 'أنت كاتب أكاديمي خبير. مهمتك هي تحسين وصقل الملخص المقدم مع مراعاة عنوان البحث.\\n\\nالمطلوب إنشاء ملخص موجز وفعال من فقرة واحدة متماسكة، دون فواصل سطرية أو حروف غير ضرورية. يجب أن يتراوح الملخص بين 100-150 كلمة وأن يتناسق بشكل جيد مع العنوان.\\n\\nمهم: قم بإنشاء الملخص المحسّن فقط. ابدأ مباشرة بعد علامة <START> وانتهِ بعلامة <END>. لا تضِف أي كلمات قبل <START> أو بعد <END>. لا تضِف أي عبارات تمهيدية أو جمل ختامية أو أي نص ليس جزءاً من محتوى الملخص الأساسي.\\n\\nإرشادات التحسين:\\n\\n- تحسين الصياغة اللغوية والأسلوب الأكاديمي\\n- التأكد من التسلسل المنطقي للأفكار\\n- تعزيز الترابط بين العنوان والملخص\\n- الحفاظ على المعلومات الأساسية مع تحسين طريقة عرضها\\n- استخدام المصطلحات الأكاديمية المناسبة\\n\\nالعنوان:\\nتسليح جيش التحرير الوطني عبر الحدود الغربية  خلال الثورة الجزائرية(1954-1962)\\n\\nالملخص الأصلي:\\nشكلت تلك الجهود والمساعي الرائدة التي قام بها قادة الثورة خلال مرحلتها الأولى (1954-1956) للبحث عن مصادر لتمويل الثورة بالأسلحة والذخيرة سواء في الداخل انطلاقا من المناطق الحدودية الشرقية والغربية (المنطقة الأولى والثانية والخامسة)بفضل الدور البارز الذي لعبه كل من: مصطفى بن بولعيد، وعباس لغرور، ولزهر شريط وعمارة العسكري(بوقلاز)ومحمد العربي بن مهيدي، أوفي الخارج بفضل جهود أحمد بن بلة، وعلي مهساس، وقاضي بشير؛ بالإضافة إلى دور محمد بوضياف الذي تكلَّف بمهمة التَّسليح على الجبهة الغربية بالتعاون والتنسيق مع محمد العربي بن مهيدي؛ الأرضية لمشروع إمداد الثورة بالسلاح والذخيرة الذي تجسدت معالمه بعد صائفة 1956،من خلال النَّشاط الحيوي لشبكات الدَّعم اللوجيستيكي في عمليات الإمداد بالأسلحة والذخيرة الذي عرفته الثورة التحريرية على الجبهتين البريَّة والبحريَّة انطلاقا من قواعد عسكرية خلفية للإمداد في طرابلس وتونس ووجده والناظور، ثم قواعد ومراكز حدودية للتموين في الشرق والغرب لتصل إلى المقاتلين في الولايات الداخلية بواسطة قوافل للتسليح عبر مسالك وممرات برية، أو خطوط إمداد بحرية تربط شبكة التَّسليح في الخارج (أوربا والمشرق العربي) ببعض الموانئ في الغرب الجزائري والسواحل المغربية. وانطلاقا من المادة التاريخية المتوفرة ،سنحاول من خلال هذه الدراسة التركيز على عمليات إمداد الثورة بالأسلحة والذخيرة على الجبهة البريَّة مع رصد أهم المسالك والممرات عبر الحدود الغربية التي شكلت المنافذ الحساسة لتهريب الأسلحة القادمة من وأوربا والمغرب..\\n\\nالملخص المحسّن:\\n<START>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_by_polishing_abstract_prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2f463049e04bc7a5d36e09af19bdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 10, 11, 11, 11, 11, 11, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:31<00:00, 57.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 11, 11, 11, 11, 11, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:01<00:00, 54.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 10, 11, 11, 11, 11, 11, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:08<00:00, 54.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 10, 11, 11, 11, 11, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:28<00:00, 56.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 9, 10, 10, 11, 11, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:03<00:00, 54.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 9, 10, 10, 11, 11, 11, 11, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [10:25<00:00, 62.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 11, 11, 11, 11, 11, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:02<00:00, 54.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 11, 11, 11, 11, 11, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [10:27<00:00, 62.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 10, 11, 11, 11, 11, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:33<00:00, 57.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 10, 10, 10, 10, 11, 11, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:16<00:00, 55.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 10, 10, 10, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:48<00:00, 58.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [08:54<00:00, 53.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 11, 11, 11, 11, 11, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:38<00:00, 57.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:36<00:00, 57.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 11, 11, 11, 11, 12, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [08:56<00:00, 53.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [10, 10, 11, 11, 11, 11, 11, 11, 12, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [07:46<00:00, 46.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 11, 11, 11, 11, 11, 12, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [08:44<00:00, 52.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [7, 9, 9, 10, 10, 10, 11, 11, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [10:00<00:00, 60.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 10, 11, 11, 11, 11, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:21<00:00, 56.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [9, 10, 10, 10, 11, 11, 11, 11, 11, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [08:52<00:00, 53.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 11, 11, 11, 11, 11, 12, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:38<00:00, 57.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 12, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [08:46<00:00, 52.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 12, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [10:12<00:00, 61.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 10, 10, 10, 11, 11, 11, 11, 12, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:21<00:00, 56.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 9, 10, 10, 10, 10, 11, 11, 11, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 11/11 [10:37<00:00, 57.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:42<00:00, 58.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 11, 11, 11, 11, 11, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:19<00:00, 55.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 11, 11, 11, 11, 11, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:13<00:00, 55.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 11, 11, 11, 11, 11, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:20<00:00, 56.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3.1-70B-Instruct - batches sizes = [8, 9, 10, 10, 10, 11, 11, 11, 11, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextGeneration using Meta-Llama-3.1-70B-Instruct: 100%|██████████| 10/10 [09:02<00:00, 54.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches processed and saved to generated_arabic_datasets/llama-batched/arabic_abstracts_dataset/by_polishing_abstracts_abstracts_generation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_abstracts(generation_prompts=generate_by_polishing_abstract_prompts, generation_type='by_polishing_abstracts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
